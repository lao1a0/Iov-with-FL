{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参数配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-21T09:16:04.065707Z",
     "start_time": "2024-02-21T09:16:03.420421Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "\n",
    "# transforms\n",
    "train_transforms = transforms.Compose([#transforms.RandomRotation(30),\n",
    "                                       # transforms.RandomResizedCrop(224),\n",
    "                                       # transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize([0.5,], [0.5,])]) # mean, std\n",
    " \n",
    "\n",
    "test_transforms = transforms.Compose([#transforms.Resize(255),\n",
    "                                      #transforms.CenterCrop(224),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.5,], [0.5,])]) # mean, std\n",
    "\n",
    "federated_train_loader = torch.utils.data.DataLoader( # <-- this is now a FederatedDataLoader \n",
    "    datasets.MNIST('/home/raoxy/data', train=True, download=True,\n",
    "                   transform=train_transforms), # <-- NEW: we distribute the dataset across all the workers, it's now a FederatedDataset\n",
    "    batch_size=200, shuffle=True, **kwargs)\n",
    "\n",
    "\n",
    "federated_test_loader = torch.utils.data.DataLoader( # <-- this is now a FederatedDataLoader \n",
    "    datasets.MNIST('/home/raoxy/data', train=False, download=True,\n",
    "                   transform=test_transforms), # <-- NEW: we distribute the dataset across all the workers, it's now a FederatedDataset\n",
    "    batch_size=200, shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-21T09:16:06.228845Z",
     "start_time": "2024-02-21T09:16:04.068017Z"
    },
    "code_folding": [
     5
    ]
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "flag=\"ALL\"\n",
    "# 定义重构后的 D_CNN 类\n",
    "if flag==\"D\":\n",
    "    class D_CNN(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(D_CNN, self).__init__()\n",
    "            # 添加一个卷积层，将输入数据的通道数从 1 增加到 32\n",
    "            self.conv0 = nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "            # 修改第一个卷积层的高度和宽度为 3\n",
    "            self.conv1 = nn.Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "            # 添加一个池化层，将输入数据的高度和宽度减半\n",
    "            self.pool0 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0)\n",
    "            # 保持第二个卷积层不变\n",
    "            self.conv2 = nn.Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
    "            # 保持第二个池化层不变\n",
    "            self.pool2 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0)\n",
    "            # 修改全连接层的输入维度为 64 * 8 * 7\n",
    "            self.fc1 = nn.Linear(64 * 8 * 7, 1000)\n",
    "            # 保持第二个全连接层不变\n",
    "            self.fc2 = nn.Linear(1000, 10)\n",
    "\n",
    "        def forward(self, x):\n",
    "            # 依次通过各个层，并使用 relu 激活函数\n",
    "            x = self.conv0(x)\n",
    "            x = nn.functional.relu(x)\n",
    "            x = self.conv1(x)\n",
    "            x = nn.functional.relu(x)\n",
    "            x = self.pool0(x)\n",
    "            x = self.conv2(x)\n",
    "            x = nn.functional.relu(x)\n",
    "            x = self.pool2(x)\n",
    "            # 将张量展平为一维向量\n",
    "            x = x.view(-1, 64 * 8 * 7)\n",
    "            x = self.fc1(x)\n",
    "            feature = nn.functional.relu(x)\n",
    "            out = self.fc2(feature)\n",
    "            # 使用 softmax 函数输出概率分布\n",
    "#             x = nn.functional.softmax(x, dim=1)\n",
    "            return out, feature\n",
    "    model = D_CNN().to(device)\n",
    "elif flag==\"L\":\n",
    "    pass\n",
    "else:\n",
    "    class D_CNN(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(D_CNN, self).__init__()\n",
    "            self.conv1 = nn.Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
    "            self.pool1 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0)\n",
    "            self.conv2 = nn.Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
    "            self.pool2 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0)\n",
    "            self.fc1 = nn.Linear(64 * 7 * 7, 1000)\n",
    "            self.fc2 = nn.Linear(1000, 10)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.conv1(x)\n",
    "            x = nn.functional.relu(x)\n",
    "            x = self.pool1(x)\n",
    "            x = self.conv2(x)\n",
    "            x = nn.functional.relu(x)\n",
    "            x = self.pool2(x)\n",
    "            x = x.view(-1, 64 * 7 * 7)\n",
    "            x = self.fc1(x)\n",
    "            feature = nn.functional.relu(x)\n",
    "            out= self.fc2(feature)\n",
    "#             x = nn.functional.softmax(x, dim=1)\n",
    "            return out, feature\n",
    "    model = D_CNN().to(device)\n",
    "    \n",
    "optimizer = optim.SGD(model.parameters(), lr=0.002,momentum=0.9) # TODO momentum is not supported at the moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-21T09:16:06.233651Z",
     "start_time": "2024-02-21T09:16:06.230586Z"
    },
    "code_folding": [
     5
    ]
   },
   "outputs": [],
   "source": [
    "def cross_entropy_for_onehot(pred, target):\n",
    "    print(pred.shape)\n",
    "    print(target.shape)\n",
    "    # 对预测值进行log_softmax操作，然后与目标值相乘，再求和，最后求平均\n",
    "    return torch.mean(torch.sum(- target * F.log_softmax(pred, dim=-1), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-21T09:16:06.254895Z",
     "start_time": "2024-02-21T09:16:06.234968Z"
    },
    "code_folding": [
     5
    ]
   },
   "outputs": [],
   "source": [
    "criterion =  nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-21T09:16:06.273392Z",
     "start_time": "2024-02-21T09:16:06.255933Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def compress_channel(data, k,flag=\"D\",batch_size=200):\n",
    "    if flag==\"D\":\n",
    "        from  ksvd import ApproximateKSVD\n",
    "        ksvd = ApproximateKSVD(n_components=k)\n",
    "        data = data.numpy()\n",
    "        output_D =  torch.empty(batch_size, 1,k, 28, dtype=torch.float, device=\"cpu\").numpy()\n",
    "        output_L = np.zeros_like(data)\n",
    "        for i in range(len(data)):\n",
    "            channel = data[i, 0, :, :]\n",
    "            output_D[i, 0, :, :] = ksvd.fit(channel).components_\n",
    "            L = ksvd.transform(channel)\n",
    "    #         output_L[i, 0, :, :]  = np.clip(L, 0, 255)\n",
    "        return torch.from_numpy(output_D)#torch.from_numpy(output_L)\n",
    "    elif flag==\"L\":\n",
    "        from  ksvd import ApproximateKSVD\n",
    "        ksvd = ApproximateKSVD(n_components=k)\n",
    "        data = data.numpy()\n",
    "        output_D =  torch.empty(batch_size, 1,k, 28, dtype=torch.float, device=\"cpu\").numpy()\n",
    "        output_L = np.zeros_like(data)\n",
    "        for i in range(len(data)):\n",
    "            channel = data[i, 0, :, :]\n",
    "            output_D[i, 0, :, :] = ksvd.fit(channel).components_\n",
    "            output_L[i, 0, :, :]  = ksvd.transform(channel)\n",
    "    #         output_L[i, 0, :, :]  = np.clip(L, 0, 255)\n",
    "        return torch.from_numpy(output_L)#torch.from_numpy(output_L)\n",
    "    else:\n",
    "        from  ksvd import ApproximateKSVD\n",
    "        ksvd = ApproximateKSVD(n_components=k)\n",
    "        data = data.numpy()\n",
    "        output_data =np.zeros_like(data)\n",
    "        for i in range(len(data)):\n",
    "            channel = data[i, 0, :, :]\n",
    "            output_D = ksvd.fit(channel).components_\n",
    "            output_L = ksvd.transform(channel)\n",
    "            output_data[i, 0, :, :] =output_L.dot(output_D)\n",
    "        return torch.from_numpy(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-21T09:16:06.296987Z",
     "start_time": "2024-02-21T09:16:06.274531Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "loss_test=[]\n",
    "acc_test=[]\n",
    "def test(model, device, federated_test_loader, batch_size):\n",
    "    global criterion,txt,loss_test,acc_test\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    n=0\n",
    "    with torch.no_grad():\n",
    "        for data, target in federated_test_loader:\n",
    "#             model.send(data.location) # <-- NEW: send the model in virtual workers to Trusted Aggregator\n",
    "            ##########################################################################\n",
    "#             target = target.to(device) \n",
    "#             data=compress_channel(data, 28)\n",
    "#             optimizer.zero_grad()\n",
    "#             output = model(data.to(device))\n",
    "            ##########################################################################\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output, feature_fc1_graph = model(data)\n",
    "            #########################################################################\n",
    "            n += target.shape[0]\n",
    "            loss = criterion(output, target.long())\n",
    "#             loss = F.nll_loss(output, target, reduction='sum')\n",
    "#             model.get()\n",
    "#             test_loss += loss.get() # sum up batch loss\n",
    "            test_loss += loss.item() # sum up batch loss\n",
    "            pred = output.argmax(1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            # print(\"{}\\t{}\".format(correct,target.shape))\n",
    "\n",
    "    test_loss /= len(federated_test_loader)\n",
    "    loss_test.append(test_loss)\n",
    "    acc_test.append(correct*1.0/n)\n",
    "\n",
    "    print('\\tTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        test_loss, correct, len(federated_test_loader) * batch_size,\n",
    "        100. * correct / n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-21T09:16:06.324086Z",
     "start_time": "2024-02-21T09:16:06.298389Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "loss_train=[]\n",
    "acc_train=[]\n",
    "\n",
    "def train(model, device, federated_train_loader, optimizer, epoch, batch_size):\n",
    "    global out,target,criterion,txt,loss_train,acc_train,deviation_f1_x_norm_sum,thresh,deviation_f1_x_norm,feature_fc1_graph\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    n=0\n",
    "    epsilon=1\n",
    "    for batch_idx, (data, target) in enumerate(federated_train_loader): # <-- now it is a distributed dataset\n",
    "        ##########################[KSVD分解]###############################\n",
    "        target = target.to(device) \n",
    "        data=compress_channel(data, 32,\"ALL\")\n",
    "        data.requires_grad = True\n",
    "        out, feature_fc1_graph = model(data.to(device))\n",
    "        ################################################################\n",
    "        deviation_f1_target = torch.zeros_like(feature_fc1_graph) # 创建一个全零的张量，用于存储目标梯度\n",
    "        deviation_f1_x_norm = torch.zeros_like(feature_fc1_graph) # 创建一个全零的张量，用于存储导数的范数\n",
    "        \n",
    "        for f in range(deviation_f1_x_norm.size(1)): # 对于每个特征向量的维度\n",
    "            deviation_f1_target[:,f] = 1 # 将目标梯度的对应位置设为1\n",
    "            feature_fc1_graph.backward(deviation_f1_target, retain_graph=True) # 对特征向量进行反向传播，计算梯度\n",
    "            deviation_f1_x = data.grad.data # 获取输入的梯度\n",
    "            deviation_f1_x = deviation_f1_x.to(device) # 获取输入的梯度\n",
    "            deviation_f1_x_norm[:,f] = torch.norm(deviation_f1_x.view(deviation_f1_x.size(0), -1), dim=1)/ torch.where(feature_fc1_graph[:, f] == 0, torch.ones_like(feature_fc1_graph[:, f]), feature_fc1_graph[:, f])\n",
    "            \n",
    "#             (feature_fc1_graph.data[:,f]+0.00001) # 计算梯度的范数与特征向量的比值\n",
    "            model.zero_grad() # 清零网络的梯度\n",
    "            data.grad.data.zero_() # 清零输入的梯度\n",
    "            deviation_f1_target[:,f] = 0 # 将目标梯度的对应位置设为0\n",
    "\n",
    "        deviation_f1_x_norm_sum = deviation_f1_x_norm.sum(axis=0) # 对每个维度求和\n",
    "        thresh = np.percentile(deviation_f1_x_norm_sum.flatten().cpu().detach().numpy(), epsilon) # 根据百分位数确定阈值\n",
    "        mask = np.where(abs(deviation_f1_x_norm_sum.cpu()) < thresh, 0, 1).astype(np.float32)\n",
    "        \n",
    "        model.fc2.weight.data = (model.fc2.weight.data.cpu()*mask).to(device)\n",
    "#         model.fc2.bias.data = (model.fc2.bias.data.cpu()*mask).to(device)\n",
    "        ################################################################\n",
    "        n += target.shape[0]\n",
    "        loss = criterion(out, target.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pred = out.argmax(1, keepdim=True) # get the index of the max log-probability\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        train_loss+=loss.item()\n",
    "\n",
    "    train_loss /= len(federated_train_loader)\n",
    "    loss_train.append(train_loss)\n",
    "    acc_train.append(correct*1.0/ n)\n",
    "\n",
    "    print('Train set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(train_loss, correct, len(federated_train_loader) * batch_size,100.*correct / n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-21T09:16:02.157Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: Average loss: 1.1779, Accuracy: 42667/60000 (71%)\n",
      "\tTest set: Average loss: 0.2934, Accuracy: 9150/10000 (92%)\n",
      "Train set: Average loss: 0.2793, Accuracy: 54953/60000 (92%)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(50):\n",
    "    train(model, device, federated_train_loader, optimizer, epoch, batch_size=200)\n",
    "    test(model, device, federated_test_loader, batch_size=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 结果保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-21T09:16:02.174Z"
    }
   },
   "outputs": [],
   "source": [
    "# save_name='LeNet-no-fed-cifar100'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-21T09:16:02.179Z"
    }
   },
   "outputs": [],
   "source": [
    "# 保存模型的权重\n",
    "# torch.save(model.state_dict(), \"/home/raoxy/model/out/{}.pt\".format(save_name))\n",
    "# print(\"保存文件：\",\"/home/raoxy/model/out/{}.pt\".format(save_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-21T09:16:02.180Z"
    }
   },
   "outputs": [],
   "source": [
    "# # 保存整个模型\n",
    "# torch.save(model, \"/home/raoxy/model/out/{}.h5\".format(save_name))\n",
    "# print(\"保存文件：\",\"/home/raoxy/model/out/{}.h5\".format(save_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-21T09:16:02.192Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "mpl.use('nbAgg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-21T09:16:02.193Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame([loss_train, loss_test, acc_train, acc_test]).T\n",
    "df.columns =['loss_train', 'loss_test','acc_train','acc_test']\n",
    "# df.to_csv(\"/home/raoxy/file/{}\".format(save_name),index=False)\n",
    "mpl.style.use('seaborn-white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-21T09:16:02.194Z"
    }
   },
   "outputs": [],
   "source": [
    "# 将DataFrame中的数据进行可视化，设置两个y轴\n",
    "ax = df[['loss_train', 'loss_test']].plot(color=['#CD0056','#F47EAB'])\n",
    "# 创建一个新的Axes对象，共享x轴\n",
    "ax2 = ax.twinx()\n",
    "# 绘制'acc_train'和'acc_test'在右侧y轴\n",
    "df[['acc_train', 'acc_test']].plot(ax=ax2, color=['#0C755F', '#A2C69B'])\n",
    "# 设置左侧y轴标签\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_xlabel('epoch')\n",
    "# 设置右侧y轴标签\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax.grid(True)\n",
    "ax2.grid(True)\n",
    "ax.legend(loc='center')\n",
    "ax2.legend(loc='center right')\n",
    "# 显示图形\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-21T09:16:02.195Z"
    },
    "code_folding": [
     11,
     42,
     66,
     107,
     126
    ]
   },
   "outputs": [],
   "source": [
    "# 导入所需的库\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision # <-- NEW: 导入torchvision库，用于处理图像数据\n",
    "import torchvision.transforms as transforms # <-- NEW: 导入transforms模块，用于对图像进行预处理\n",
    "\n",
    "device=\"cuda\"\n",
    "# 定义LeNet模型，参考[3]\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        x = self.fc1(x)\n",
    "        feature = nn.functional.relu(x)\n",
    "        out= self.fc2(feature)\n",
    "        return out\n",
    "\n",
    "# 创建一个LeNet实例\n",
    "model = LeNet().to(device)\n",
    "# 定义损失函数，使用交叉熵损失\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# 定义优化器，使用Adam优化器，设置学习率为0.001 # <-- NEW: 使用Adam优化器，可以自适应地调整学习率，提高收敛速度\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 定义图像的预处理方法，包括转换为张量，归一化，以及一些数据增强的操作，如随机旋转，随机裁剪，随机翻转等 # <-- NEW: 使用数据增强的方法，可以增加数据的多样性，提高模型的泛化能力\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomRotation(10), # 随机旋转10度以内\n",
    "    transforms.RandomResizedCrop(28), # 随机裁剪为28*28的大小\n",
    "    transforms.RandomHorizontalFlip(), # 随机水平翻转\n",
    "    transforms.ToTensor(), # 转换为张量\n",
    "    transforms.Normalize((0.5,), (0.5,)) # 归一化，设置均值和标准差为0.5\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(), # 转换为张量\n",
    "    transforms.Normalize((0.5,), (0.5,)) # 归一化，设置均值和标准差为0.5\n",
    "])\n",
    "\n",
    "# 加载CNN数据集，这里使用torchvision.datasets.MNIST方法，可以自动下载和加载数据，并且应用预处理方法 # <-- NEW: 使用torchvision.datasets.MNIST方法，可以方便地加载数据，无需手动下载和读取\n",
    "train_data = torchvision.datasets.MNIST(root='/home/raoxy/data', train=True, download=True, transform=train_transforms)\n",
    "test_data = torchvision.datasets.MNIST(root='/home/raoxy/data', train=False, download=True, transform=test_transforms)\n",
    "\n",
    "# 定义一个批次的大小，这里假设为32\n",
    "batch_size = 32\n",
    "# 使用torch.utils.data.DataLoader方法，将数据集分为多个批次，设置打乱顺序和多线程加载 # <-- NEW: 使用torch.utils.data.DataLoader方法，可以方便地分批次，打乱顺序和多线程加载数据，提高效率\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "# 定义一个训练的函数，输入为一个批次的数据和标签\n",
    "def train_batch(data, labels):\n",
    "    # 将模型设为训练模式\n",
    "    model.train()\n",
    "    # 将数据和标签移动到合适的设备上，比如GPU\n",
    "    data = data.to(device)\n",
    "    labels = labels.to(device)\n",
    "    # 清零优化器的梯度\n",
    "    optimizer.zero_grad()\n",
    "    # 前向传播，得到模型的输出\n",
    "    outputs = model(data)\n",
    "    # 计算损失\n",
    "    loss = criterion(outputs, labels)\n",
    "    # 反向传播，计算梯度\n",
    "    loss.backward()\n",
    "    # 在最后一层全连接层的梯度上，执行以下操作\n",
    "    # 使用torch.norm，计算这一层的L2范数，将计算结果除以这一层的输入进行归一化\n",
    "    # 然后计算得到的和，再使用np.percentile，对原始梯度进行裁剪\n",
    "    # 这里假设裁剪的百分位数为90\n",
    "    # 获取最后一层全连接层的权重和偏置的梯度\n",
    "    weight_grad = model.fc2.weight.grad\n",
    "    bias_grad = model.fc2.bias.grad\n",
    "    # 计算L2范数\n",
    "    norm = torch.norm(weight_grad, p=2)\n",
    "    # 将计算结果除以这一层的输入进行归一化\n",
    "    norm = norm / (batch_size * 84)\n",
    "    # 计算得到的和\n",
    "    sum = norm + torch.sum(bias_grad).item()\n",
    "    # 使用np.percentile，对原始梯度进行裁剪，这里使用detach().numpy()将张量转换为numpy数组\n",
    "    model.fc2.weight.grad = torch.tensor(weight_grad, dtype=model.fc2.weight.grad.dtype, device=model.fc2.weight.grad.device)\n",
    "    model.fc2.bias.grad = torch.tensor(bias_grad, dtype=model.fc2.bias.grad.dtype, device=model.fc2.bias.grad.device)\n",
    "\n",
    "\n",
    "    # 将裁剪后的梯度重新赋值给模型的参数\n",
    "    model.fc2.weight.grad = torch.tensor(weight_grad).to(device)\n",
    "    model.fc2.bias.grad = torch.tensor(bias_grad).to(device)\n",
    "    # 使用优化器更新模型的参数\n",
    "    optimizer.step()\n",
    "    # 返回损失的值\n",
    "    return loss.item\n",
    "\n",
    "# 定义一个测试的函数，输入为一个批次的数据和标签\n",
    "def test_batch(data, labels):\n",
    "    # 将模型设为评估模式\n",
    "    model.eval()\n",
    "    # 将数据和标签移动到合适的设备上，比如GPU\n",
    "    data = data.to(device)\n",
    "    labels = labels.to(device)\n",
    "    # 前向传播，得到模型的输出\n",
    "    outputs = model(data)\n",
    "    # 计算损失\n",
    "    loss = criterion(outputs, labels)\n",
    "    # 计算准确率，使用torch.max得到每个样本的预测类别，然后和真实类别比较，计算正确的个数\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    acc = torch.sum(preds == labels).item() / batch_size\n",
    "    # 返回损失和准确率的值\n",
    "    return loss.item(), acc\n",
    "\n",
    "# 定义训练的轮数，这里假设为10\n",
    "epochs = 10\n",
    "# 遍历每一轮\n",
    "for epoch in range(epochs):\n",
    "    # 初始化训练集的总损失和总样本数\n",
    "    train_loss = 0.0\n",
    "    train_count = 0\n",
    "    # 遍历训练集的每一个批次\n",
    "    for data, labels in train_loader: # <-- NEW: 使用迭代器的方式，直接从数据加载器中获取数据和标签\n",
    "        # 调用训练的函数，得到损失的值\n",
    "        loss = train_batch(data.to(device), labels.to(device))\n",
    "        # 累加损失和样本数\n",
    "        train_loss += loss.item()\n",
    "        train_count += len(data)\n",
    "    # 计算训练集的平均损失\n",
    "    train_loss = train_loss / train_count\n",
    "    # 打印训练集的结果\n",
    "    print(f'Epoch {epoch + 1}, Train Loss: {train_loss:.4f}')\n",
    "\n",
    "    # 初始化测试集的总损失，总准确率和总样本数\n",
    "    test_loss = 0.0\n",
    "    test_acc = 0.0\n",
    "    test_count = 0\n",
    "    # 遍历测试集的每一个批次\n",
    "    for data, labels in test_loader: # <-- NEW: 使用迭代器的方式，直接从数据加载器中获取数据和标签\n",
    "        # 调用测试的函数，得到损失和准确率的值\n",
    "        loss, acc = test_batch(data, labels)\n",
    "        # 累加损失，准确率和样本数\n",
    "        test_loss += loss.item()\n",
    "        test_acc += acc\n",
    "        test_count += len(data)\n",
    "    # 计算测试集的平均损失和平均准确率\n",
    "    test_loss = test_loss / test_count\n",
    "    test_acc = test_acc / len(test_loader)\n",
    "    # 打印测试集的结果\n",
    "    print(f'Epoch {epoch + 1}, Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "785px",
    "left": "120px",
    "top": "110.525px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
