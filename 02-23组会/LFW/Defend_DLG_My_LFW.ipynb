{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-21T03:24:49.362113Z",
     "start_time": "2024-02-21T03:24:48.371406Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "NWa7Xo6PkIl3",
    "outputId": "9d406a09-f23e-4b89-c3f2-a6dd4e9d6a2a",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0+cu118 0.15.1+cu118\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import grad\n",
    "import torchvision\n",
    "from torchvision import models, datasets, transforms\n",
    "\n",
    "torch.manual_seed(50)\n",
    "\n",
    "print(torch.__version__, torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-21T03:25:53.534999Z",
     "start_time": "2024-02-21T03:25:53.506716Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 99,
     "referenced_widgets": [
      "c3ca367dae49422fac361cb0b12604c7",
      "648bccaeee754d08ab55501e40c804ea",
      "c1614b2739dd4f44b5ad0e4793b7a5d8",
      "58ea7f96451c48db8aacc1479ce5a570",
      "d9f1b39155c64eb6a33d745a27f940be",
      "b2682540a10b4646ba96e3cc549f2a23",
      "19c8d63413164e4ea00a5c3dbd9f2b15",
      "f1d4dec7f3cf4aa7b321dbdcb49e705a"
     ]
    },
    "id": "VjKWqs2akepH",
    "outputId": "4da32114-194a-4642-8036-3d3e574cf17c",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# 加载CIFAR-10数据集，如果本地没有则自动下载\n",
    "dst = datasets.LFWPeople(\"/home/raoxy/data\",download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-21T03:26:05.436205Z",
     "start_time": "2024-02-21T03:26:05.431814Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 99,
     "referenced_widgets": [
      "c3ca367dae49422fac361cb0b12604c7",
      "648bccaeee754d08ab55501e40c804ea",
      "c1614b2739dd4f44b5ad0e4793b7a5d8",
      "58ea7f96451c48db8aacc1479ce5a570",
      "d9f1b39155c64eb6a33d745a27f940be",
      "b2682540a10b4646ba96e3cc549f2a23",
      "19c8d63413164e4ea00a5c3dbd9f2b15",
      "f1d4dec7f3cf4aa7b321dbdcb49e705a"
     ]
    },
    "id": "VjKWqs2akepH",
    "outputId": "4da32114-194a-4642-8036-3d3e574cf17c",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda\n"
     ]
    }
   ],
   "source": [
    "# 定义一个转换序列，包括缩放、裁剪、转换为张量等操作\n",
    "tp = transforms.Compose([\n",
    "    transforms.Resize(32),\n",
    "    transforms.CenterCrop(32),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# 定义一个函数，用于将张量转换为PIL图像\n",
    "tt = transforms.ToPILImage()\n",
    "\n",
    "# 检测是否有可用的GPU，如果有则使用GPU，否则使用CPU\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "print(\"Running on %s\" % device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-21T03:26:10.412298Z",
     "start_time": "2024-02-21T03:26:10.404503Z"
    },
    "code_folding": [],
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 99,
     "referenced_widgets": [
      "c3ca367dae49422fac361cb0b12604c7",
      "648bccaeee754d08ab55501e40c804ea",
      "c1614b2739dd4f44b5ad0e4793b7a5d8",
      "58ea7f96451c48db8aacc1479ce5a570",
      "d9f1b39155c64eb6a33d745a27f940be",
      "b2682540a10b4646ba96e3cc549f2a23",
      "19c8d63413164e4ea00a5c3dbd9f2b15",
      "f1d4dec7f3cf4aa7b321dbdcb49e705a"
     ]
    },
    "id": "VjKWqs2akepH",
    "outputId": "4da32114-194a-4642-8036-3d3e574cf17c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 定义一个函数，用于将标签转换为one-hot编码的张量\n",
    "def label_to_onehot(target, num_classes=10):\n",
    "    # 在第一个维度上增加一个维度，使得target的形状为(N, 1)\n",
    "    target = torch.unsqueeze(target, 1)\n",
    "    # 创建一个全零的张量，形状为(N, num_classes)，设备与target相同\n",
    "    onehot_target = torch.zeros(target.size(0), num_classes, device=target.device)\n",
    "    # 在第二个维度上根据target的值将onehot_target的对应位置设为1\n",
    "    onehot_target.scatter_(1, target, 1)\n",
    "    # 返回onehot_target\n",
    "    return onehot_target\n",
    "\n",
    "# 定义一个函数，用于计算one-hot编码的标签和预测值之间的交叉熵损失\n",
    "def cross_entropy_for_onehot(pred, target):\n",
    "    # 对预测值进行log_softmax操作，然后与目标值相乘，再求和，最后求平均\n",
    "    return torch.mean(torch.sum(- target * F.log_softmax(pred, dim=-1), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-21T03:27:12.137606Z",
     "start_time": "2024-02-21T03:27:09.288451Z"
    },
    "code_folding": [],
    "id": "AorI020iVjjS",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LeNet(\n",
       "  (body): Sequential(\n",
       "    (0): Conv2d(3, 12, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "    (1): Sigmoid()\n",
       "    (2): Conv2d(12, 12, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "    (3): Sigmoid()\n",
       "    (4): Conv2d(12, 12, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (5): Sigmoid()\n",
       "    (6): Conv2d(12, 12, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (7): Sigmoid()\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=5749, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义一个类，继承自nn.Module，表示LeNet模型\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self,out_features):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.body = nn.Sequential(\n",
    "            nn.Conv2d(3, 12, kernel_size=5, stride=2, padding=2),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Conv2d(12, 12, kernel_size=5, stride=2, padding=2),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Conv2d(12, 12, kernel_size=5, stride=1, padding=2),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Conv2d(12, 12, kernel_size=5, stride=1, padding=2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(768, out_features, bias=True) # 修改输出特征数为 5749\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.body(x)\n",
    "        x = x.view(-1, 768)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "# 创建一个LeNet模型的实例，并将其移动到设备上，可以是CPU或GPU\n",
    "net = LeNet(5749).to(device)\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-21T03:27:15.126894Z",
     "start_time": "2024-02-21T03:27:15.122978Z"
    },
    "id": "AorI020iVjjS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 定义一个函数，用于初始化模型的权重和偏置\n",
    "def weights_init(m):\n",
    "    # 如果模型有权重属性，就将其均匀分布在[-0.5, 0.5]之间\n",
    "    if hasattr(m, \"weight\"):\n",
    "        m.weight.data.uniform_(-0.5, 0.5)\n",
    "    # 如果模型有偏置属性，也将其均匀分布在[-0.5, 0.5]之间\n",
    "    if hasattr(m, \"bias\"):\n",
    "        m.bias.data.uniform_(-0.5, 0.5)\n",
    "        \n",
    "    \n",
    "# 调用weights_init函数，对模型的参数进行初始化\n",
    "net.apply(weights_init)\n",
    "# 定义损失函数为one-hot编码的交叉熵损失\n",
    "criterion = cross_entropy_for_onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 图像索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-21T05:35:41.052878Z",
     "start_time": "2024-02-21T03:36:14.581266Z"
    },
    "code_folding": [
     1
    ],
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# KSVD\n",
    "def K_Soteria(img_index):\n",
    "    global History\n",
    "    from PIL import Image\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    gt_data = dst[img_index][0] # 从数据集中获取图像，并使用之前定义的转换序列处理图像\n",
    "    img_array = np.array(gt_data)\n",
    "    \n",
    "    # 分离三个颜色通道\n",
    "    R = img_array[:, :, 0]\n",
    "    G = img_array[:, :, 1]\n",
    "    B = img_array[:, :, 2]\n",
    "\n",
    "    # 定义一个函数，用SVD对单通道矩阵进行压缩\n",
    "    def compress_channel(data, k):\n",
    "        from  ksvd import ApproximateKSVD\n",
    "        ksvd = ApproximateKSVD(n_components=k)\n",
    "        data = data.astype(np.float32)  # 将 data 转换为 float32 类型\n",
    "        output_data =np.zeros_like(data)\n",
    "        output_D = ksvd.fit(data).components_\n",
    "        output_L = ksvd.transform(data)\n",
    "        output_data =output_L.dot(output_D)\n",
    "        return output_data\n",
    "\n",
    "    k=33\n",
    "    # 对每个颜色通道进行压缩\n",
    "    R_k = compress_channel(R, k)\n",
    "    G_k = compress_channel(G, k)\n",
    "    B_k = compress_channel(B, k)\n",
    "    \n",
    "    # 将压缩后的三个颜色通道合并为一个三维数组\n",
    "    img_array_k = np.stack((R_k, G_k, B_k), axis=2)\n",
    "    # 将压缩后的数组转换为图片\n",
    "    img_k = Image.fromarray(img_array_k.astype('uint8'))\n",
    "    # 从数据集中获取图像，并使用之前定义的转换序列处理图像\n",
    "    gt_data = tp(img_k).to(device)\n",
    "\n",
    "    # 将图像的形状调整为(1, 3, 32, 32)，表示批量大小为1，通道数为3，高度和宽度为32\n",
    "    gt_data = gt_data.view(1, *gt_data.size())\n",
    "    # 从数据集中获取图像的标签，并转换为长整型张量\n",
    "    gt_label = torch.Tensor([dst[img_index][1]]).long().to(device)\n",
    "    # 将标签的形状调整为(1,)，表示批量大小为1\n",
    "    gt_label = gt_label.view(1, )\n",
    "    # 将标签转换为one-hot编码的张量，形状为(1, 10)，表示批量大小为1，类别数为10\n",
    "    gt_onehot_label = label_to_onehot(gt_label, num_classes=10)\n",
    "    # 设置图像的梯度属性为True，表示可以对图像进行梯度计算\n",
    "    gt_data.requires_grad = True\n",
    "\n",
    "    # 计算输出和特征向量相对于输入的导数的范数与特征向量的范数的比值，即||dr/dX||/||r||\n",
    "    out,feature_fc1_graph = net(gt_data) # 通过网络得到输出和特征向量\n",
    "    deviation_f1_target = torch.zeros_like(feature_fc1_graph) # 创建一个全零的张量，用于存储目标梯度\n",
    "    deviation_f1_x_norm = torch.zeros_like(feature_fc1_graph) # 创建一个全零的张量，用于存储导数的范数\n",
    "    for f in range(deviation_f1_x_norm.size(1)): # 对于每个特征向量的维度\n",
    "        deviation_f1_target[:,f] = 1 # 将目标梯度的对应位置设为1\n",
    "        feature_fc1_graph.backward(deviation_f1_target, retain_graph=True) # 对特征向量进行反向传播，计算梯度\n",
    "        deviation_f1_x = gt_data.grad.data # 获取输入的梯度\n",
    "        deviation_f1_x_norm[:,f] = torch.norm(deviation_f1_x.view(deviation_f1_x.size(0), -1), dim=1)/(feature_fc1_graph.data[:,f]) # 计算梯度的范数与特征向量的比值\n",
    "        net.zero_grad() # 清零网络的梯度\n",
    "        gt_data.grad.data.zero_() # 清零输入的梯度\n",
    "        deviation_f1_target[:,f] = 0 # 将目标梯度的对应位置设为0\n",
    "\n",
    "    # 根据最小的||dr_i/dX||/||r_i||来剪枝特征向量\n",
    "    deviation_f1_x_norm_sum = deviation_f1_x_norm.sum(axis=0) # 对每个维度求和\n",
    "    thresh = np.percentile(deviation_f1_x_norm_sum.flatten().cpu().numpy(), 1) # 根据百分位数确定阈值\n",
    "    mask = np.where(abs(deviation_f1_x_norm_sum.cpu()) < thresh, 0, 1).astype(np.float32) # 根据阈值生成掩码，小于阈值的为0，大于阈值的为1\n",
    "    y = criterion(out, gt_onehot_label) # 计算输出和标签之间的损失\n",
    "    dy_dx = torch.autograd.grad(y, net.parameters()) # 计算损失对网络参数的梯度\n",
    "\n",
    "    # 与其他客户端共享梯度\n",
    "    original_dy_dx = list((_.detach().clone() for _ in dy_dx)) # 复制梯度\n",
    "    original_dy_dx[-2] = original_dy_dx[-2] * torch.Tensor(mask).to(device)# 将梯度乘以掩码，实现剪枝\n",
    "\n",
    "    # 生成一些随机的数据和标签，形状与真实的数据和标签相同\n",
    "    dummy_data_init = torch.randn(gt_data.size()) # 使用torch.randn()函数[^1^][1]生成一个服从标准正态分布的张量\n",
    "    dummy_label_init = torch.randn(gt_onehot_label.size()) # 同上\n",
    "\n",
    "    # 将随机的数据和标签转换为张量，并移动到设备上，可以是CPU或GPU\n",
    "    dummy_data = torch.Tensor(dummy_data_init).to(device).requires_grad_(True) # 使用torch.Tensor()函数将numpy数组转换为张量，并设置requires_grad属性为True，表示可以对数据进行梯度计算\n",
    "    dummy_label = torch.Tensor(dummy_label_init).to(device).requires_grad_(True) # 同上\n",
    "\n",
    "    # 定义一个LBFGS优化器，将虚拟的数据和标签作为需要优化的参数\n",
    "    optimizer = torch.optim.LBFGS([dummy_data, dummy_label] )\n",
    "    # 注释掉了另一个优化器，SGD，可能是为了比较效果\n",
    "    #optimizer = torch.optim.SGD([dummy_data, dummy_label], lr=0.1, momentum=0.9 )\n",
    "\n",
    "    # 创建一个空列表，用于存储优化过程中的虚拟数据\n",
    "    history = []\n",
    "    # 定义一个变量，用于存储虚拟数据和真实数据之间的最小均方误差\n",
    "    MSE_min = 100\n",
    "    # 进行300次迭代优化\n",
    "    for iters in range(300):\n",
    "        # 定义一个闭包函数，用于计算虚拟数据和标签的梯度差\n",
    "        def closure():\n",
    "            # 清零优化器的梯度\n",
    "            optimizer.zero_grad()\n",
    "            # 通过网络得到虚拟数据的预测和特征向量\n",
    "            #out, [feature_fc1_graph, feature_fc2_graph, feature_fc3_graph] = net(gt_data)\n",
    "            pred, f1 = net(dummy_data) \n",
    "            # 对虚拟标签进行softmax操作，得到one-hot编码的张量\n",
    "            dummy_onehot_label = F.softmax(dummy_label, dim=-1)\n",
    "            # 计算预测和虚拟标签之间的损失，使用之前定义的交叉熵损失函数\n",
    "            dummy_loss = criterion(pred, dummy_onehot_label) # TODO: fix the gt_label to dummy_label in both code and slides.\n",
    "            # 计算损失对网络参数的梯度，创建计算图\n",
    "            dummy_dy_dx = torch.autograd.grad(dummy_loss, net.parameters(), create_graph=True)\n",
    "\n",
    "            # 初始化梯度差和梯度数量为0\n",
    "            grad_diff = 0\n",
    "            grad_count = 0\n",
    "\n",
    "            i = 0\n",
    "            for gx, gy in zip(dummy_dy_dx, original_dy_dx): # TODO: fix the variablas here\n",
    "                # 只计算前100个参数的梯度差，可能是为了节省计算资源\n",
    "                if i <=100:\n",
    "                    grad_diff += ((gx - gy) ** 2).sum()\n",
    "                    grad_count += gx.nelement()\n",
    "                i += 1\n",
    "            # 计算梯度差的平均值，乘以一个系数，可能是为了调整梯度的大小\n",
    "            #grad_diff = grad_diff / grad_count * 1000\n",
    "\n",
    "            # 计算特征向量和真实数据的差的平方和，可能是另一种计算梯度差的方法\n",
    "            #grad_diff = ((feature_fc1_graph - f1) ** 2).sum()\n",
    "            # 对梯度差进行反向传播，计算虚拟数据和标签的梯度\n",
    "            grad_diff.backward()\n",
    "\n",
    "            # 返回梯度差\n",
    "            return grad_diff\n",
    "\n",
    "        # 调用优化器的step()方法，使用闭包函数进行一步优化\n",
    "        optimizer.step(closure)\n",
    "        # 如果当前的均方误差小于之前的最小值，就更新最小值\n",
    "        if MSE_min > (gt_data[0] - dummy_data[0]).pow(2).mean().item():\n",
    "            MSE_min = (gt_data[0] - dummy_data[0]).pow(2).mean().item()\n",
    "        # 每隔10次迭代，打印当前的迭代次数，梯度差，均方误差和最小均方误差\n",
    "        if iters % 10 == 0: \n",
    "            current_loss = closure()\n",
    "            # print(\"{}, loss: {}, MSE: {}, MSE_min: {}\".format(iters, current_loss.item(), (gt_data[0] - dummy_data[0]).pow(2).mean().item(), MSE_min))\n",
    "        # 将当前的虚拟数据添加到历史列表中\n",
    "        history.append(tt(dummy_data[0].cpu()))\n",
    "    Print(History)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Print(History):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.subplots_adjust(wspace=0.05, hspace=0.05)\n",
    "    for j in range(min(50,len(History))):\n",
    "        plt.subplot(5, 10, j+1)\n",
    "        plt.imshow(tt(tp(History[j])))\n",
    "        plt.title(\"title={}\".format(j))\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-21T05:35:41.056824Z",
     "start_time": "2024-02-21T05:35:41.054634Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../aten/src/ATen/native/cuda/ScatterGatherKernel.cu:365: operator(): block: [0,0,0], thread: [0,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuDNN error: CUDNN_STATUS_INTERNAL_ERROR",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m History\u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m50\u001b[39m):\n\u001b[0;32m----> 4\u001b[0m     History\u001b[38;5;241m.\u001b[39mappend(\u001b[43mK_Soteria\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(i)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 51\u001b[0m, in \u001b[0;36mK_Soteria\u001b[0;34m(img_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m gt_data\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# 计算输出和特征向量相对于输入的导数的范数与特征向量的范数的比值，即||dr/dX||/||r||\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m out,feature_fc1_graph \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgt_data\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# 通过网络得到输出和特征向量\u001b[39;00m\n\u001b[1;32m     52\u001b[0m deviation_f1_target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(feature_fc1_graph) \u001b[38;5;66;03m# 创建一个全零的张量，用于存储目标梯度\u001b[39;00m\n\u001b[1;32m     53\u001b[0m deviation_f1_x_norm \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(feature_fc1_graph) \u001b[38;5;66;03m# 创建一个全零的张量，用于存储导数的范数\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[5], line 20\u001b[0m, in \u001b[0;36mLeNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 20\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m768\u001b[39m)\n\u001b[1;32m     22\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(x)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuDNN error: CUDNN_STATUS_INTERNAL_ERROR"
     ]
    }
   ],
   "source": [
    "History=[]\n",
    "\n",
    "for i in range(50):\n",
    "    History.append(K_Soteria(i)[-1])\n",
    "    print(\"#\"*50+\"[\"+ str(i)+\"]\"+\"#\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-21T06:12:21.035029Z",
     "start_time": "2024-02-21T06:12:20.281268Z"
    }
   },
   "outputs": [],
   "source": [
    "# 画图\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplots_adjust(wspace=0.05, hspace=0.05)\n",
    "\n",
    "for i in range(50):\n",
    "    plt.subplot(5, 10, i + 1)\n",
    "    plt.imshow(dst[i][0])\n",
    "#     plt.title((dir_label[dst[i][1]]))\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-21T06:48:00.081005Z",
     "start_time": "2024-02-21T06:47:59.072396Z"
    },
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# 画图\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplots_adjust(wspace=0.05, hspace=0.05)\n",
    "\n",
    "for i in range(50):\n",
    "    plt.subplot(5, 10, i + 1)\n",
    "    plt.title(\"{}\".format(i))\n",
    "    plt.imshow(History[i])\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-21T06:23:57.312517Z",
     "start_time": "2024-02-21T06:23:39.240882Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    plt.axis('off')\n",
    "    plt.imshow(History[i])\n",
    "    plt.savefig('image_dlg_f_{}.png'.format(i), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-21T06:40:14.869897Z",
     "start_time": "2024-02-21T06:39:56.923677Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    plt.axis('off')\n",
    "    plt.imshow(dst[i][0])\n",
    "    plt.savefig('image_r_f_{}.png'.format(i), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-20T10:15:04.050520Z",
     "start_time": "2024-02-20T10:15:04.050511Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "psnr(np.array(dst[img_index][0]), np.array(history[-1]))\n",
    "\n",
    "# 导入必要的库\n",
    "from skimage.metrics import structural_similarity as ssim # 使用skimage库中的SSIM函数\n",
    "import numpy as np # 使用numpy库进行数组操作\n",
    "\n",
    "# 定义SSIM计算函数\n",
    "def compare_ssim(img1, img2, maxvalue=255):\n",
    "    # 将图像转换为浮点数类型\n",
    "    img1 = np.array(img1).astype(np.float64)\n",
    "    img2 = np.array(img2).astype(np.float64)\n",
    "    # 调用skimage库中的SSIM函数，指定数据范围为maxvalue\n",
    "    return ssim(img1, img2, data_range=maxvalue,win_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-20T10:15:04.051667Z",
     "start_time": "2024-02-20T10:15:04.051658Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "compare_ssim(dst[img_index][0], history[-1], maxvalue=255)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Deep Leakage from Gradients.ipynb",
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "209.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "19c8d63413164e4ea00a5c3dbd9f2b15": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "58ea7f96451c48db8aacc1479ce5a570": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f1d4dec7f3cf4aa7b321dbdcb49e705a",
      "placeholder": "​",
      "style": "IPY_MODEL_19c8d63413164e4ea00a5c3dbd9f2b15",
      "value": " 170500096/? [00:19&lt;00:00, 75905876.79it/s]"
     }
    },
    "648bccaeee754d08ab55501e40c804ea": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b2682540a10b4646ba96e3cc549f2a23": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c1614b2739dd4f44b5ad0e4793b7a5d8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b2682540a10b4646ba96e3cc549f2a23",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d9f1b39155c64eb6a33d745a27f940be",
      "value": 1
     }
    },
    "c3ca367dae49422fac361cb0b12604c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c1614b2739dd4f44b5ad0e4793b7a5d8",
       "IPY_MODEL_58ea7f96451c48db8aacc1479ce5a570"
      ],
      "layout": "IPY_MODEL_648bccaeee754d08ab55501e40c804ea"
     }
    },
    "d9f1b39155c64eb6a33d745a27f940be": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "f1d4dec7f3cf4aa7b321dbdcb49e705a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
