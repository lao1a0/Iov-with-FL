# 工作安排：实验

- [ ] 复现：Auditing Privacy Defenses in Federated Learning via Generative Gradient Leakage

> [⁡⁤‍‌﻿⁣⁢⁡⁢﻿⁡⁣﻿⁢⁣⁢⁣⁣﻿⁣﻿⁣⁢‍⁤⁡‌⁡‬﻿⁢⁢⁢⁤⁡⁡‌⁡‬⁤Auditing Privacy Defenses in Federated Learning via Generative Gradient Leakage - 飞书云文档 (feishu.cn)](https://z1q7kn87r2y.feishu.cn/docx/D9Z0d1nEFo5nBlxwznxcOXjBnZg)

- [ ] 复现：GradViT: Gradient Inversion of Vision Transformers

>  [⁣⁣⁣⁣⁤‍⁡⁣‍‍⁢﻿‌⁣‬⁤‬﻿‌‌⁣﻿⁢‌⁡‬⁣⁣‬﻿‬⁡‌‌﻿‌⁡⁤‌⁢‌GradViT: Gradient Inversion of Vision Transformers - 飞书云文档 (feishu.cn)](https://z1q7kn87r2y.feishu.cn/docx/SKwudqmSUoJXlBxcveWcRMANn8d)

- [ ] 复现：Using Highly Compressed Gradients in Federated Learning for Data Reconstruction Attacks

> [HCGLA(2022 ) - 飞书云文档 (feishu.cn)](https://z1q7kn87r2y.feishu.cn/docx/MDAWdmtUtomoS0xH4aHcWDFfnPe)

- [ ] 复现：Dreaming to Distill: Data-free Knowledge Transfer via DeepInversion

> [‍⁢⁡‌‌‌‍⁤⁤‌‌‬‬⁢‌⁡‬⁤‬﻿⁡⁤‬⁤‍⁤‌‍⁢⁤⁢‬‌‍⁣⁤‬⁤⁤Dreaming to Distill: Data-free Knowledge Transfer via DeepInversion（CVPR 2020） - 飞书云文档 (feishu.cn)](https://z1q7kn87r2y.feishu.cn/docx/QCX3dRIytoC8QXxTqRHc1NPQnib)

## GradInversion

- [x] 复现：See through Gradients: Image Batch Recovery via GradInversion

> [‌⁢‍⁡⁡‬⁤‬‌⁣﻿﻿‍⁢⁤‍‌‍‌‍﻿⁢‍‬‍⁤‌⁤⁢⁢⁤‌⁢⁢‬⁡‬⁣‬⁢⁣‍GradInversion（CVPR 2021 ） - 飞书云文档 (feishu.cn)](https://z1q7kn87r2y.feishu.cn/docx/Cb1Ed5tEUom4WyxebLrcVEuDn4c)

[See through Gradients - Optimization-based Attack - ResNet50 on ImageNet - Jupyter Notebook](http://59.65.191.29:8888/notebooks/breaching-main/examples/See through Gradients - Optimization-based Attack - ResNet50 on ImageNet.ipynb)

[Iov-with-FL/12-22组会/See through Gradients - Optimization-based Attack - ResNet50 on ImageNet.ipynb at main · lao1a0/Iov-with-FL (github.com)](https://github.com/lao1a0/Iov-with-FL/blob/main/12-22组会/See through Gradients - Optimization-based Attack - ResNet50 on ImageNet.ipynb)

## Inverting Gradients

- [x] 复现：Inverting Gradients -- How easy is it to break privacy in federated learning?

> [‍‌‌‍⁤⁤⁢‬⁡⁡⁣⁣⁣⁣‌﻿⁢⁡⁣⁤‍⁡‍‬⁡‬⁢⁢⁡‌⁤⁢‍﻿‌‬‬⁤⁡‬⁤Inverting gradients - 飞书云文档 (feishu.cn)](https://z1q7kn87r2y.feishu.cn/docx/G7uTdmbByo62R2xeUsMcEl8Rnxc)

代码：

[Inverting Gradients - Optimization-based Attack - Large Batch CIFAR-100 - Jupyter Notebook](http://59.65.191.29:8888/notebooks/breaching-main/examples/Inverting Gradients - Optimization-based Attack - Large Batch CIFAR-100.ipynb)

[Inverting Gradients - Optimization-based Attack - ResNet18 on ImageNet - Federated Averaging - Jupyter Notebook](http://59.65.191.29:8888/notebooks/breaching-main/examples/Inverting Gradients - Optimization-based Attack - ResNet18 on ImageNet - Federated Averaging.ipynb)

[Inverting Gradients - Optimization-based Attack - ResNet18 on ImageNet - Jupyter Notebook](http://59.65.191.29:8888/notebooks/breaching-main/examples/Inverting Gradients - Optimization-based Attack - ResNet18 on ImageNet.ipynb)

## iDLG

- [x] 复现：iDLG: Improved Deep Leakage from Gradients

> [⁣⁣‬⁡⁤﻿⁣⁣⁤⁡‍⁢‬⁤‌‌﻿⁣‍⁡‬‌⁤‍﻿⁤﻿‬⁣⁡⁢⁣⁣⁢‌﻿‬⁣⁣‬‍⁤⁣iDLG - 飞书云文档 (feishu.cn)](https://z1q7kn87r2y.feishu.cn/docx/MseNdO89PoybxbxUdwJcdOAUnOc)

[Iov-with-FL/12-15组会 at main · lao1a0/Iov-with-FL (github.com)](https://github.com/lao1a0/Iov-with-FL/tree/main/12-15组会)

## DLG

- [x] 复现：Deep Leakage from Gradients

> [‌‍⁣‍⁢⁣﻿‌⁡⁣‬⁤﻿﻿⁢⁣⁣⁣‍‌‍‍⁢⁤‬﻿‌⁢‍‌⁤⁡⁡‌⁣⁣‬⁣DLG（NeurIPS 2019） - 飞书云文档 (feishu.cn)](https://z1q7kn87r2y.feishu.cn/docx/Zv5YdoirZoiPRuxzTg4clejqnog)

[Deep Leakage from Gradients - Optimization-based Attack - ConvNet CIFAR-10 - Jupyter Notebook](http://59.65.191.29:8888/notebooks/breaching-main/examples/Deep Leakage from Gradients - Optimization-based Attack - ConvNet CIFAR-10.ipynb)

# 工作安排：论文

- [ ] 写完第2章
- [ ] 写完研究现状
- [ ] 看完5篇硕士论文



《GradViT: Gradient Inversion of Vision Transformers》《See through Gradients: Image Batch Recovery via GradInversion》《iDLG: Improved Deep Leakage from Gradients》，《Deep Leakage from Gradients》



《Auditing Privacy Defenses in Federated Learning via Generative Gradient Leakage》，《Using Highly Compressed Gradients in Federated Learning for Data Reconstruction Attacks》，《Dreaming to Distill: Data-free Knowledge Transfer via DeepInversion》，《Inverting Gradients -- How easy is it to break privacy in federated learning?》，

# 相关备忘录

## 评价指标：

1. **MSE (Mean Squared Error):**
   - 均方误差，表示攻击后图像与原始图像之间的平均像素差的平方。**数值越小**表示还原的图像与原始图像越相似。

2. **PSNR (Peak Signal-to-Noise Ratio):**
   - 峰值信噪比，用于度量还原图像与原始图像之间的相似性。**数值越大**表示还原的图像质量越高，因为PSNR 与图像相似性成反比。

3. **FMSE (Feature-level Mean Squared Error):**
   - 特征级别的均方误差，表示攻击后图像与原始图像在某些特征层面上的平均差异。**数值越小**表示还原的图像在特征级别上越接近原始图像。
4. **LPIPS (Learned Perceptual Image Patch Similarity):**
   - 学习的感知图像块相似度，用于度量图像的感知相似性。**数值越小**表示还原的图像在感知上越接近原始图像。
5. **R-PSNR (Relative Peak Signal-to-Noise Ratio):**
   - 相对峰值信噪比，是一种对比攻击后图像与原始图像之间峰值信噪比的相对指标。**数值越大**表示还原的图像相对于原始图像在质量上的改进。
6. **IIP-pixel (Inverted Image Pixel):**
   - 像素翻转指标，表示攻击后图像中像素的翻转百分比。在这里，数值为 100.00% 表示攻击后的图像完全颠倒，即每个像素的值都被反转。
   - 在攻击场景中，像素翻转是指攻击后的图像中像素值相对于原始图像被反转或翻转的情况。具体来说，对于每个像素，原始图像中的白色像素（值较大）在攻击后可能变成黑色像素（值较小），反之亦然。这个指标对于分析图像中的细微变化和攻击造成的影响是有用的。在某些应用中，像素翻转可能被视为一种不希望的攻击效果，因为它可能导致图像中的信息完全颠倒，从而改变图像的语义和含义。在其他情况下，像素翻转可能是攻击者有意引入的一种变化，用于混淆模型或使其对图像内容产生误导。
7. **SSIM (Structural Similarity Index):**
   - 结构相似性指数，用于度量还原图像与原始图像之间的结构相似性。数值范围在 -1 到 1 之间，**越接近 1 表示还原的图像结构与原始图像越相似**。
8. **max R-PSNR (Max Relative Peak Signal-to-Noise Ratio):**
   - 最大相对峰值信噪比，表示攻击后图像与原始图像之间峰值信噪比的最大相对值。衡量了图像的还原程度。max R-PSNR 计算了攻击后图像相对于原始图像在峰值信噪比上的最大提升。**数值越大**，表示攻击后图像相对于原始图像在峰值信噪比方面的改进越显著。
9. **max SSIM (Max Structural Similarity Index):**
   - 最大结构相似性指数，表示攻击后图像与原始图像之间结构相似性的最大值。
10. **Label Acc (Label Accuracy):**
       - 标签准确率，表示攻击后图像上的标签准确率。在这里，数值为 100.00% 表示攻击后的图像标签全部正确。


## 配置详细：

Model architecture resnet18 loaded with 11,173,962 parameters and 9,620 buffers. 

- 加载了一个resnet18模型，有 11,173,962 个参数和 9,620 个缓冲区。

Overall this is a data ratio of 909:1 for target shape [4, 3, 32, 32] given that num_queries=1. 

- 给定 num_queries=1 的情况下，目标形状为 [4, 3, 32, 32] 的数据比例为 909:1。目标形状是指要重建的图像的维度，4 代表批量大小，3 代表通道数，32 代表高度和宽度。数据比例是指模型参数和图像像素的比值，越大表示重建难度越高。

---
User (of type UserMultiStep) with settings: 定义了一个类型为 UserMultiStep 的用户，它有以下的设置：
- Number of data points: 4 用户拥有的数据点的数量，即图像的数量，为 4。
- Threat model: 用户在联合学习中的安全假设：
    - User provides labels: True 
      - 用户提供了图像的标签，即类别信息，为 True。
    - User provides buffers: False
      - 用户没有提供缓冲区，即模型的中间输出，为 False。
    - User provides number of data points: True 
      - 用户提供了数据点的数量，为 True。

Data: 用户的数据信息： 
- Dataset: CIFAR10 用户的数据集是CIFAR10。
- user: 1 用户的编号是 1。

Local FL Setup: 用户的本地联合学习设置：
- Number of local update steps: 4 
    - 用户在每次通信轮中使用本地数据训练模型的次数为 4。
- Data per local update step: 2 
    - 用户在每次本地更新步中训练的图像数量为 2。
- Local learning rate: 0.001 
    - 用户的本地学习率，即训练模型时的梯度下降的步长，为 0.001。

Threat model: 用户的本地威胁模型：
- Share these hyperparams to server: True 
  - 用户是否将这些超参数（本地更新步数，数据量，学习率）分享给服务器，为 True。

---
Server (of type HonestServer) with settings: 定义了一个类型为 HonestServer 的服务器：
- Threat model: Honest-but-curious
    - 服务器的威胁模型，即服务器会按照协议执行联合学习，但是会尝试从用户的梯度中提取信息，为 Honest-but-curious。
- Number of planned queries: 1 
    - 服务器计划的查询次数，即服务器从用户处获取梯度的次数，为 1。
- Has external/public data: False 
    - 服务器是否拥有外部或公开的数据，为 False。
- Model: 服务器的模型信息：
    - model specification: resnet18 
      - 服务器的模型规范，即使用的模型类型，为 resnet18。
    - model state: default 
      - 服务器的模型状态，即模型的初始权重，为 default，表示使用随机初始化的权重。
    - public buffers: True 
      - 服务器是否公开缓冲区，即模型的中间输出，为 True。

Secrets: {} 服务器的秘密信息，即服务器想要从用户的梯度中恢复的图像，为一个空的字典，表示没有指定任何秘密。

---

Attacker (of type OptimizationBasedAttacker) with settings: 定义了一个类型为 OptimizationBasedAttacker 的攻击者：
- Hyperparameter Template: invertinggradients 
    - 攻击者的超参数模板，即攻击者使用的梯度反演方法，为 invertinggradients，使用基于优化的梯度反演方法。
- Objective: Cosine Similarity with scale=1.0 and task reg=0.0  攻击者的目标函数，即攻击者使用的重建图像的损失函数，为余弦相似度：
    - scale: 1.0 
        - 损失函数的缩放因子，即损失函数的权重，为 1.0。
    - task reg: 0.0 
        - 任务正则化的系数，即重建图像的分类准确率的权重，为 0.0，表示不使用任务正则化。
- Regularizers: Total Variation, scale=0.001. p=1 q=1. 攻击者的正则化项，即攻击者使用的重建图像的平滑性约束，为总变分：
    - scale: 0.001 
      - 正则化项的缩放因子，即正则化项的权重，为 0.001。
    - p: 1 
      - 正则化项的 p 范数的指数，为 1，表示使用 L1 范数。
    - q: 1 
      - 正则化项的 q 范数的指数，为 1，表示使用 L1 范数。
- Augmentations: 攻击者的数据增强，即攻击者使用的重建图像的变换方式，为空，表示不使用数据增强。
- Optimization Setup: 攻击者的优化设置，即攻击者使用的重建图像的优化方法：
    - optimizer: adam 
      - 攻击者的优化器，即攻击者使用的梯度下降的算法，为 adam 自适应矩估计的方法。
    - signed: hard 
      - 攻击者的符号策略，即攻击者使用的梯度的符号处理方式，为 hard，表示使用硬符号，即只保留梯度的符号，忽略梯度的大小。
    - step_size: 0.1
      - 攻击者的步长，即攻击者使用的梯度下降的步长，为 0.1。
    - boxed: True 
      - 攻击者的边界约束，即攻击者使用的重建图像的取值范围，为 True，表示使用边界约束，即限制重建图像的像素值在 [0, 1] 之间。
    - max_iterations: 24000
      - 攻击者的最大迭代次数，即攻击者使用的梯度下降的总次数，为 24000。
    - step_size_decay: step-lr
      - 攻击者的步长衰减，即攻击者使用的梯度下降的步长变化方式，为 step-lr，表示使用阶梯式的学习率衰减，即每隔一定的迭代次数，将步长乘以一个衰减因子。
    - langevin_noise: 0.0
      - 攻击者的朗之万噪声，即攻击者使用的梯度下降的随机扰动，为 0.0，表示不使用朗之万噪声。
    - warmup: 0 
      - 攻击者的预热步数，即攻击者使用的梯度下降的初始步长增加的次数，为 0，表示不使用预热。
    - grad_clip: None
      - 攻击者的梯度裁剪，即攻击者使用的梯度下降的梯度大小限制，为 None，表示不使用梯度裁剪。
    - callback: 1000 
      - 攻击者的回调间隔，即攻击者使用的梯度下降的每隔多少次迭代，输出重建图像的次数，为 1000。
