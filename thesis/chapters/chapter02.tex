 \setlength{\baselineskip}{20pt}
\chapter{研究现状}
\label{cha:chap2}

本章主要介绍基于联邦学习的车联网异常检测系统的研究现状，其中包括车联网异常检测，梯度泄露攻击、梯度泄露防御的相关研究以及目前存在的问题。

\section{车联网异常检测的研究现状}

\subsection{基于普通深度学习算法的IDS}

本节介绍了一些基于普通深度学习算法的车联网入侵检测系统，包括基于卷积神经网络（CNN）和基于卷积长短期记忆网络（ConvLSTM）的方法。这些方法主要利用了车联网中的链路负载数据和网络消息数据，来提取特征并进行分类或异常检测。这些方法的优点是能够有效地捕捉数据的时空特征，提高检测的准确性和鲁棒性。这些方法的缺点是没有考虑数据的隐私保护和模型的计算效率，可能存在泄露敏感信息或增加计算负担的风险。

Nie等人\cite{Date_Driven}提出了一种基于CNN的车联网入侵检测系统，该系统基于路边单元（RSU）的链路负载数据，来检测针对RSU的入侵行为。该系统使用了一个多层CNN架构，来提取链路负载数据的时空特征，并输出一个二分类结果，表示是否存在入侵。该系统还对CNN的收敛性进行了理论分析，证明了其在实际部署中的可行性。

Alladi等人\cite{ref13}提出了一种基于人工智能的车联网入侵检测架构，该架构部署在多接入边缘计算（MEC）服务器上，利用车辆流量数据，来检测车联网中的异常事件。该架构提出了两种基于CNN的分类技术，一种是直接基于时间序列的分类，另一种是基于序列图像的分类。两种技术都将车辆流量数据转换为一维或二维的输入，然后通过CNN进行特征提取和分类。实验结果表明，基于序列图像的分类技术表现最佳。该架构的优点是具有成本效益，因为可以利用现有的MEC服务器，无需额外的基础设施投资。Alladi等人\cite{ref14}在\cite{ref13}的基础上，进一步探索了不同的深度学习模型在车联网入侵检测中的表现，包括CNN、循环神经网络（RNN）、长短期记忆网络（LSTM）和门控循环单元（GRU）。他们设计了三种不同的分类方案，分别是单阶段粗粒度分类、单阶段细粒度分类和两阶段分类。实验结果表明，单阶段细粒度分类方案在所有模型中表现最佳，两阶段分类方案也有相当的性能。他们还提出了一种基于应用场景和数据可用性的模型选择方法，可以根据不同的需求，选择最合适的模型进行部署。

Yang等人\cite{Federated-AI-Enabled-IDS}提出了一种基于ConvLSTM的车联网入侵检测方法，该方法利用了车联网中的网络消息数据，来检测针对智能车辆（ICV）的入侵行为。该方法使用了一个ConvLSTM模型，来提取网络消息数据的时空特征，并输出一个多分类结果，表示不同类型的入侵。为了训练ConvLSTM模型，该方法采用了一种联合学习（FL）框架，其中ICV作为本地客户端，MEC服务器作为参数服务器。该方法还开发了一种基于近端策略优化（PPO）的联邦客户端选择（FCS）方案，来优化FL框架的性能，提高训练的效率和隐私保护。

\subsection{基于高级深度学习算法的IDS}

本节介绍了一些基于高级深度学习算法的车联网入侵检测系统，包括基于LSTM和GRU的混合模型、基于树模型的集成方法、基于CNN的迁移学习和集成学习方法、基于Transformer的变体模型和基于BERT的预训练模型。这些方法主要利用了车联网中的网络消息数据和车辆流量数据，来提取特征并进行分类或异常检测。这些方法的优点是能够有效地处理高维、非线性、时序的数据，提高检测的准确性和鲁棒性。这些方法的缺点是没有考虑数据的隐私保护和模型的计算效率，可能存在泄露敏感信息或增加计算负担的风险。

Ullah等人\cite{ref15}提出了一种基于LSTM和GRU的混合深度学习模型，用于车联网入侵检测。该模型结合了LSTM和GRU的优势，能够捕捉数据的长期和短期依赖性，并减少训练和响应时间。Yang等人\cite{ref16}提出了一种基于树模型的智能入侵检测系统，适用于自动驾驶汽车的CAN总线和普通车联网。该系统使用了决策树、随机森林、额外树和XGBoost等树模型，能够处理高维、非线性的数据，并提高检测的速度和效果。之后，Yang等人\cite{A_Transfer_Learning_and_Optimized_CNN_Based_Intrusion_Detection_System_for_Internet_of_Vehicles}又提出基于CNN和超参数优化技术，提出了一种基于迁移学习和集成学习的车联网入侵检测系统。该系统利用了迁移学习的思想，将预训练的CNN模型迁移到车联网的数据上，并使用超参数优化技术来调整模型的参数。该系统还使用了集成学习的方法，将多个CNN模型的输出进行融合，以提高检测的稳定性和准确性。实验结果表明，该系统的检测率和f1值均超过99.25\%。同时，Yang等人\cite{ref18}还提出了一种新的集成IDS框架LCCDE，用于检测各种类型的网络攻击。该框架选择了XGBoost、LightGBM和CatBoost这三种表现最好的机器学习模型，构建了一个多分类器。该框架还利用了类领袖模型的预测置信度，来做出准确的决策，以提高检测的可靠性和效率。

Wu等人\cite{RTIDS}提出了一种具有鲁棒性的基于Transformer的入侵检测系统RTIDS。该系统使用了一个变体的堆栈编码器-解码器神经网络，从高维原始数据中学习低维特征表示。该系统还利用了自注意力机制，来促进网络流量类型的分类，以提高检测的灵敏度和精确度。Alkhatib等人\cite{can_ids}提出了一种名为CAN-BERT的基于深度学习的网络入侵检测系统，用于检测对CAN总线协议的网络攻击，提高CAN总线协议的安全性。该系统使用了一个预训练的BERT模型，来提取CAN消息数据的语义特征，并输出一个二分类结果，表示是否存在入侵。该系统的优点是能够实时地检测网络攻击，减少延迟和误报。

\subsection{基于于数据隐私保护的IDS}

本节介绍了一些关注于数据隐私保护的车联网入侵检测系统，包括基于联邦学习和拉格朗日编码的方法。这些方法主要利用了车联网中的CAN数据和流量数据，来检测智能网联汽车的网络入侵。这些方法的优点是能够在保护数据隐私的同时，提高检测的准确性和安全性。这些方法的缺点是需要额外的计算资源和通信开销，可能影响检测的效率和实时性。

Yu等人\cite{Federated-LSTM}提出了一种基于联邦学习的LSTM模型，用于检测智能网联汽车的网络入侵。该模型基于车联网系统模型和CAN数据框架，利用车辆之间的协作学习，来提取CAN数据的时序特征，并输出一个二分类结果，表示是否存在入侵。该模型通过仿真实验，验证了其对各种类型的攻击的检测精度超过90％。

Hbaieb等人\cite{Federated_Learning_Based_IDS_Approach_for_the_IoV}提出了一种基于联邦学习的入侵检测系统，旨在为车联网提供安全解决方案。该系统部署在软件定义网络的结构下，利用车辆的流量数据，来检测车联网中的异常事件。该系统集成了信任指标，以帮助保护车联网的安全。该系统还利用车辆的数据包丢弃率和节点身份，来进行协作学习，提高检测的效果。

Ni等人\cite{Lagrange_Coded_Federated_Learning}提出了一种新方法，用于提高车联网系统中联邦学习的准确性和安全性。该方法基于拉格朗日编码计算的概念，为车辆处理的数据引入了计算冗余，以抵抗潜在的攻击和故障。该方法设计了一个名为L-CofL的拉格朗日编码的FL模型，支持将LCC与FL集成，以实现安全的车联网系统。该方法通过理论分析和实验评估，证明了其在准确性和安全性方面的优势。

\section{梯度泄露攻击的研究现状}
\subsection{基于优化的梯度泄露攻击研究现状}

随着深度神经网络的广泛应用，分布式学习系统（如协同学习、联邦学习等）也越来越受到关注。这些系统通过在多个客户端之间交换梯度信息，实现了在不共享原始数据的情况下协同训练一个神经网络。然而，近年来的研究表明，梯度信息可能会泄露客户端的私有数据，从而威胁到用户的隐私安全。

Hitaj等人\cite{Deep_Models_Under_the_GAN}是基于优化方法的先驱，他们在2017年提出了生成对抗网络泄露攻击DMU-GAN，利用生成对抗网络（GAN）从梯度中重建出训练集中的数据分布。他们的方法可以处理多个输入点的情况，但需要访问一个与训练集相似的辅助数据集。并且，尽管DMU-GAN可以生成类似于私有数据集的数据，但它无法提取单个数据点。在2019年，Zhu等人\cite{DLG}提出了深度梯度泄露（Deep Leakage from Gradients，DLG），首次展示了从公开共享的梯度中恢复出私有训练数据的可能性。他们的方法利用梯度匹配的思想，通过优化一个虚拟数据和对应的标签，使其与真实数据产生的梯度尽可能相似。他们的方法不需要知道输入的类别标签，也不需要任何辅助数据集，且适用于任何可微分的模型和损失函数。他们在计算机视觉和自然语言处理的任务上验证了其有效性，表明其攻击能够实现像素级的精确重建和词汇级的匹配。然而，DLG的方法也存在一些局限性，如收敛困难、标签发现不稳定、对梯度压缩不敏感等。为了克服DLG的局限性，一些改进的方法被提出。Zhao等人\cite{iDLGID}在2020年提出了改进的深度梯度泄露（Improved Deep Leakage from Gradients，iDLG），针对梯度压缩的情况，重新设计了优化目标函数，使得压缩后的虚拟梯度与压缩后的真实梯度相似。他们还设计了一种新的虚拟数据初始化方法，以补偿梯度压缩造成的信息损失。他们的方法可以从高度压缩的梯度（0.1\%压缩率）中恢复出准确的数据和标签，而DLG的方法只能支持70\%的压缩率，从而实现了700倍的改进。

在梯度反演的攻击方法方面。Yin等人\cite{DeepInversion}在2020年提出深度反演（DeepInversion），利用梯度优化的方法，从随机噪声中生成自然图像，同时匹配梯度、图像先验和块间的总变分正则化。此外，使用自适应深度反演，通过最大化教师和学生网络对数之间的Jensen-Shannon散度，来提高合成图像的多样性。他们在CIFAR-10和ImageNet数据集上训练的网络的合成图像显示出高保真度和真实感，并帮助实现了一种新的无数据应用——不需要任何真实图像或标注数据的应用。在这之后第二年，Yin等人\cite{GradInversion}提出梯度反演（GradInversion），针对视觉变换器（Vision Transformer，ViT）的情况，利用梯度优化的方法，从随机噪声中生成自然图像，同时匹配梯度、图像先验和块间的总变分正则化。他们的方法在ImageNet1K和MS-Celeb-1M数据集上展示了高保真度和真实性的图像重建，且发现视觉变换器比之前研究的卷积神经网络更容易受到梯度反演攻击的影响。另一种新颖的方法是，Hatamizadeh等人\cite{GradViT}在2022年提出的GradViT，针对大批量数据的情况，利用梯度无关的优化方法（如进化策略和贝叶斯优化），从梯度中重建出高质量的图像。他们的方法在复杂的数据集、深层的网络和大批量的情况下，都能实现与原始数据的高保真度和接近度，且提出了一种用于衡量梯度泄露量的实证工具。

在真实的FL场景中，原始全梯度的直接传输非常消耗资源，因此参与者更希望在将局部梯度发送回服务器之前压缩局部梯度，以减少通信开销\cite{Gradient_Leakage_Attacks_in_Federated_Learning}。针对梯度压缩的情况，一种有效的数据重建攻击是Yang等人\cite{HCGLA}在2023年提出来的HCGLA，它针对DLG的不合理的优化目标函数，重新设计了一个合理的目标函数，使得压缩后的虚拟梯度与压缩后的真实梯度相似。该方法还设计了一种新的虚拟数据初始化方法，以补偿梯度压缩造成的信息损失。该方法可以从高度压缩的梯度（0.1\%压缩率）中恢复出准确的数据和标签，而DLG的方法只能支持70\%的压缩率，从而实现了700倍的改进。

针对联邦学习的情况，Geiping等人\cite{Inverting_Gradients}基于iDLG，在2020年提出了一种从梯度中恢复输入数据的数值重构方法IG，将梯度泄露攻击带入联邦学习领域。他们利用一个幅度不变的损失函数，来解决梯度操作的非线性问题。他们还证明了任何输入到一个全连接层的数据都可以解析地重构，而与剩余的网络结构无关。他们的方法在真实的深度和非光滑的网络上展示了从梯度中重构输入数据的可能性，即使在多个迭代或多张图像上进行梯度平均的情况下。一种新的隐私泄露方式是Li等热\cite{GGL}在2022年提出来的GGL，利用从公共图像数据集中学习的GAN的潜在空间作为先验，来补偿梯度降级过程中造成的信息损失。该方法还探索了各种无梯度优化方法，如进化策略和贝叶斯优化，来提高梯度反演的效果。该方法希望作为一种工具，用于实证地测量隐私泄露的量，从而促进更强大的防御机制的设计。

综上所述，梯度反演的问题是深度学习中的一个重要的安全挑战，它涉及到多个方面的研究，如攻击方法、防御方法、模型结构、应用场景等。这些研究对于理解和保护深度学习中的隐私具有重要的意义和价值。然而，目前的研究还存在一些不足和局限，如梯度反演的理论分析、梯度反演的评估标准、梯度反演的通用方法等。因此，梯度反演的问题还需要进一步的探索和深入的研究。

\section{梯度泄露防御的研究现状}

联邦学习能保护用户隐私和数据安全。但是这里不可避免地又带来了联邦学习自身的数据泄露风险，正如Yang等人\cite{Gradient_Leakage_Attacks_in_Federated_Learning}提到需要进一步研究 GLA 防御策略。目前，两种主要的防御方法包括向梯度添加高斯噪声和实施基于密码的方法。然而，前者是以模型准确性为代价的，找到平衡点仍然具有挑战性。后者不仅需要修改模型结构，而且在更新全局模型时会消耗大量的带宽和存储资源。针对梯度泄露的防御方法，主要有以下几种：（1）在共享梯度之前添加噪声，如差分隐私、同态加密等，但这些方法\cite{Efficient_Privacy-Preserving_Federated_Learning_Against_Inference_Attacks_for_IoT,Mixed_Quantization_Enabled_Federated_Learning_To_Tackle_Gradient_Inversion_Attacks,Revealing_and_Protecting_Labels_in_Distributed_Training,FedML-HE_An_Efficient_Homomorphic-Encryption-Based_Privacy-Preserving_Federated_Learning_System,Distributed_Learning_in_Trusted_Execution_Environment_A_Case_Study_of_Federated_Learning_in_SGX}会降低梯度的有效性，影响模型的训练效果；（2）使用梯度压缩，如量化、稀疏化、低秩近似等，但这些方法并不能完全阻止梯度泄露，只能提高攻击的难度；（3）使用梯度掩码，如梯度修剪、梯度过滤等，但这些方法可能会损失梯度的重要信息，导致模型的性能下降；（4）使用梯度混淆，如梯度扰动、梯度混合等，但这些方法可能会引入额外的计算开销，且需要设计合适的混淆策略。

\subsection{基于梯度变换的梯度防御方案}

基于梯度变换的主要思想是在联邦学习中对梯度进行一定的变换，以降低梯度泄露的风险。变换的方式包括压缩、量化、稀疏化、扰动等。这类方法的优点是可以有效地减少梯度的信息量，从而提高隐私保护的水平。缺点是变换可能会影响梯度的质量，从而降低模型的性能。此外，一些变换也可能引入额外的计算或通信开销。一些研究\cite{An_Optimized_Sparse_Response_Mechanism_for_Differentially_Private_Federated_Learning,PrivateDL_Privacy-preserving_collaborative_deep_learning_against_leakage_from_gradient_sharing}使用基于梯度稀疏化和差分隐私的防御方法，使得保持模型性能的同时，提供可量化的隐私保证。而另一些\cite{Gradient-Leakage_Resilient_Federated_Learning,DataLens_Scalable_Privacy_Preserving_Training_via_Gradient_Compression_and_Aggregation}则采用基于梯度压缩和聚合的防御方法，在保持模型性能的同时，实现可扩展的梯度隐私保护，并保证模型的收敛性和准确性。梯度稀疏化是指只选择部分重要的梯度进行传输或更新，从而减少梯度的维度和通信开销而差分隐私是指在梯度上添加随机噪声，从而保证梯度的统计特性不会泄露单个样本的信息。这两种方法的结合可以提高模型的隐私保护水平，但也会带来一定的性能损失。梯度量化是指将连续的梯度值映射到离散的集合中，从而减少梯度的位数和存储空间而随机化是指在梯度上引入一定的随机性，从而减少梯度的冗余和相关性。这两种方法的结合可以提高模型的训练效率和通信效率，但也会带来一定的精度损失。总的来说，梯度稀疏化和差分隐私的防御方法主要关注模型的隐私安全性，而梯度量化和随机化的防御方法主要关注模型的训练速度。Jing Wu等人\cite{Concealing_Sensitive_Samples_against_Gradient_Leakage_in_Federated_Learning}认为模型反演攻击主要利用了梯度信息中包含的数据信息，因此提出了DCS2——一种通过生成隐蔽样本来混淆敏感数据梯度的方法。隐蔽样本是指在梯度层面与敏感数据相似，但在视觉层面与敏感数据不同的样本，这样可以使得攻击者无法从梯度中重建敏感数据，同时又不影响联邦学习的性能。隐蔽样本地生成使用余弦相似度进行衡量，最大化隐蔽样本和敏感样本之间的不相似性，而使用GEM技术进行梯度投影和改进mixup正则以保证加入的隐藏样本与原始小批量梯度对齐，以便服务器能够有效地更新模型参数。Chen等人\cite{QP-LDP_for_better_global_model_performance_in_federated_learning}提出的QP-LDP算法一种基于梯度量化和局部差分隐私的防御方法，可以在提供隐私保证的同时，改善全局模型的性能。具体的来说，作者将整个算法分为了三个部分，梯度量化部分采用了一种基于聚类的量化方法，将梯度向量划分为若干个子向量，然后对每个子向量进行聚类，得到聚类中心和聚类标签。聚类中心表示了梯度的主要方向，聚类标签表示了梯度的细节信息。作者只需要共享聚类中心和聚类标签，而不需要共享原始的梯度向量，从而实现了梯度的压缩。为了增加梯度的不确定性，提高隐私保护的水平，在第二部分中采用一种基于拉普拉斯噪声的扰动方法，将噪声添加到聚类中心和聚类标签上。最后采用了一种基于加权平均的聚合方法，将来自不同客户端的量化和扰动后的梯度进行聚合，得到全局模型的更新梯度。

\subsection{基于梯度修剪的梯度防御方案}

基于梯度修剪的防御方法的主要思想是在联邦学习中对梯度进行修剪，以降低梯度泄露的风险。修剪的方式包括剪枝、对齐、过滤等。这类方法的优点是可以有效地减少梯度的敏感度，从而提高隐私保护的水平。缺点是修剪可能会影响梯度的有效性，从而降低模型的性能。此外，一些修剪也可能引入额外的计算或通信开销。PriPrune\cite{PriPrune_Quantifying_and_Preserving_Privacy_in_Pruned_Federated_Learning}是一种结合了梯度剪枝和量化的防御方法，旨在提高用户数据的隐私保护。它的基本思想是：在每轮联合学习中，用户设备先对自己的本地模型进行修剪，去除一些不重要的参数或连接；然后，用户设备对修剪后的模型进行量化，即将参数的值映射到一些离散的数值，从而进一步降低模型的精度和敏感性； 最后，用户设备将量化后的模型上传到服务器，服务器对不同用户的模型进行聚合，得到一个全局的模型，并将其发送回用户设备。它的不足之处在于需要用户设备具备一定的计算能力和存储空间，因为修剪和量化都需要在用户设备上进行，而且需要保存修剪和量化的信息。并且还需要服务器和用户设备之间有一定的信任度，因为服务器需要知道用户设备的修剪和量化的参数和方法，以便正确地聚合模型，而用户设备需要信任服务器不会滥用或泄露自己的模型。Zhiqiu Z等人\cite{Preserving_data_privacy_in_federated_learning_through_large_gradient_pruning}提出了一种基于梯度大幅剪枝的防御方法，可以在保护数据隐私的同时，提高联邦学习的效率。具体来说，作者提出了两种防御机制，分别是：（1）严格的大梯度剪枝（SLGP）：这种机制将每一层的梯度按绝对值大小降序排序，并将大于一个阈值的梯度剪枝为零。这样可以减少梯度中包含的私有数据信息，从而防止对手利用梯度匹配的方法来重建数据。阈值的选择可以根据一些指标，如PSNR，来自动调整，以满足隐私保护的要求。（2）宽松的大梯度剪枝（RLGP）：这种机制不是直接将大梯度剪枝为零，而是根据每一层梯度的$l_2$范数值来对大梯度进行不同程度的剪枝。如果$l_2$范数值大于1，那么大梯度会被该值除以，从而缩小其大小；如果$l_2$范数值小于等于1，那么大梯度仍然会被剪枝为零。这样可以避免对一些参数较少的模型造成不稳定的训练过程。这篇论文的防御方法需要选择合适的剪枝阈值，但是这个阈值可能会影响模型的效果和隐私保护的程度，而且这个阈值可能需要根据不同的数据集和模型来调整，没有给出一个通用的方法。Jiahui H等人\cite{Shield_Against_Gradient_Leakage_Attacks_Adaptive_Privacy-Preserving_Federated_Learning}提出的AdpPPFL框架可为联邦学习提供自适应的隐私保护功能。具体的来说，在每一轮通信$t$中，1）服务器运行泄露风险感知的隐私分解机制，获取分配给本轮的隐私预算$ϵ_t$以及分配给$K$个随机选择的客户端的隐私预算$\{\epsilon_{t}^{i}\}_{i=1}^{K}$；2）服务器向客户端广播全局模型$w^{t-1}$和$\{\epsilon_{i}^{i}\}_{i=1}^{K}$；3）每个客户端$i$采用自适应隐私保护的本地训练机制进行本地模型训练，然后将生成的模型更新$w^t_i$上传到服务器进行聚合。

\subsection{基于梯度扰动的梯度防御方案}

基于梯度扰动的防御方法的主要思想是在联邦学习中对梯度进行扰动，以降低梯度泄露的风险。扰动的方式包括加噪、旋转、变换等。这类方法的优点是可以有效地增加梯度的不确定性，从而提高隐私保护的水平。缺点是扰动可能会影响梯度的方向，从而降低模型的性能。此外，一些扰动也可能引入额外的计算或通信开销。例如，Soteria\cite{Soteria}是一种联邦学习中的隐私防御方法，它通过在神经网络的某一层添加扰动，降低数据的可重构性，同时保持模型的训练效果。设 $H$ 是一个有 $L$ 层的神经网络，每层的大小为 $n_1,\ldots,n_L$，输入层的大小为 $n_0$。令 $X$ 和 $X^{\prime}\in\mathbb{R}^{n_0}$ 分别表示 $H$ 的原始输入和重建输入， $h_{i,j}: \mathbb{R} ^{n_i}\to \mathbb{R} ^{n_j}$ 表示 $H$ 的第 $i$ 层到第 $j$ 层之间的映射函数。对于选定的防御层 $l$，用 $r= h_{0, l- 1}( X) $ 和 $r^{\prime}= h_{0, l- 1}( X^{\prime})$ 分别表示 $X$ 和 $X^{\prime}$ 在该层的中间表示，并求解以下优化问题：$\max_{r'}||X-X'||_2\text{ s.t. }||r-r'||_0\leq\epsilon.$。该问题的目标是找到一个最小的扰动 $r^{\prime}-r$，使得重建输入 $X^{\prime}$ 与原始输入 $X$ 的差异最大。由于攻击者无法直接观察到 $r$ 和 $r^{\prime}$，这相当于在防御层 $l$ 上生成了一个扰动的梯度。因此，Soteria 的客户端不再发送原始的梯度 $\nabla W$，而是发送扰动的梯度 $\nabla W^{\prime}$，其中 $\nabla W_l^{\prime}$ 是根据扰动的中间表示 $r^{\prime}$ 计算的。PGL\cite{Protect_Privacy_from_Gradient_Leakage_Attack_in_Federated_Learning}也是一种在本地客户端进行随机梯度扰动的方案，不同与上者，PGL直接在梯度上进行的扰动而不是只扰动表示层，使用梯度相对于输入的雅可比矩阵来反映隐私泄露风险。Yongqi J等人\cite{Directional_Privacy_for_Deep_Learning}提出了一种新颖的隐私保护联邦学习方法，它采用了双重扰动的策略。首先，它在条件生成对抗网络（CGAN）的目标函数中加入了一个特征提取器和一个模糊函数，以提高生成数据的质量和多样性。然后，它将生成数据和真实数据混合，形成虚假的训练数据，用于更新中心模型，从而减少真实数据的泄露风险。其次，它对全连接层的梯度进行了扰动，方法是用一个与梯度维度相同的矩阵进行 Hadamard 乘积。这个矩阵是根据一个目标函数生成的，该目标函数旨在使得推断出的特征表示尽可能接近真实特征表示，而重建出的数据尽可能远离真实数据。通过这种方法，它可以有效地抵抗推断攻击，同时保持较高的准确率。更多的扰动方案还有，Fei W等人\cite{More_than_Enough_is_Too_Much_Adaptive_Defenses_against_Gradient_Leakage_in_Production_Federated_Learning}提出来的Outpost、Xue Y等人\cite{An_Accuracy-Lossless_Perturbation_Method_for_Defending_Privacy_Attacks_in_Federated_Learning} 提出的联邦学习模型扰动方法。

\subsection{基于数据变换的梯度防御方案}

基于数据变换的防御方法的核心思想是在联邦学习的过程中对数据进行不同形式的变换，从而减少梯度泄露的可能性。这些变换包括对数据进行分解、转换、扩展等操作。这种方法的优势在于能够有效地提升数据的多样性，进而增强隐私保护的效果。然而，这种方法也存在一些局限性，例如变换可能会损害数据的质量，导致模型的性能下降。另外，一些变换还可能增加计算或通信的成本。例如，Wei G等人\cite{Automatic_Transformation_Search_Against_Deep_Leakage_From_Gradients}提出了一种名为ATS的方案，它利用随机搜索的方法从数据增强库中选择最佳的变换策略。Jing L等人\cite{Safeguard_the_Original_Data_in_Federated_Learning_via_Data_Decomposition}则使用稀疏字典学习（DL）或QR分解的技术对原始数据进行隐私保护的处理。PRECODE\cite{PRECODE_A_Generic_Model_Extension_to_Prevent_Deep_Gradient_Leakage}采用变分建模的方式，在模型的某一层添加一个随机采样的瓶颈层，从而有效地阻断梯度泄露的途径。Yuting M等人\cite{Privacy-preserving_Collaborative_Learning_with_Scalable_Image_Transformation_and_Autoencoder}设计了一种基于图像变换和自编码器的隐私保护协同学习方案，它能够在多个数据源之间共享一个预测任务，同时提升机器学习模型的性能，防止敏感数据被梯度重构攻击泄露。该方案通过调节块大小，对训练图像进行随机排列，破坏图像的空间结构和语义信息。自编码器则用于从变换后的图像中学习有用的特征表示，以适应高维图像的分类任务。

综上所述，梯度泄露是一个严重的隐私威胁，需要引起人们的高度重视。目前的研究还存在一些不足，如攻击方法的通用性、防御方法的有效性、梯度泄露的理论分析等，需要进一步的探索和完善。

\section{本章小结}

本章介绍了车联网异常检测和梯度泄露的研究现状，分为两个部分。首先介绍了车联网数据安全的入侵检测系统（IDS）的发展现状，从基于普通深度学习算法的IDS、基于高级深度学习算法的IDS和关注于数据隐私保护的IDS三个角度进行分析。其次，介绍了车联网模型安全的梯度泄露的发展现状，在攻击方面介绍了基于优化的梯度泄露攻击，而防御方面介绍了基于梯度变换的梯度防御策略、基于梯度修剪的梯度防御方案、基于梯度扰动的梯度防御方案和基于数据变换的梯度防御方案4个方面。最后，本章指出了目前的研究存在的一些不足和局限，如梯度反演的理论分析、梯度反演的评估标准、梯度反演的通用方法等，需要进一步的探索和完善。