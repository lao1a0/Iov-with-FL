{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参数配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T06:58:17.442614Z",
     "start_time": "2024-02-28T06:58:15.634038Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "\n",
    "# transforms\n",
    "train_transforms = transforms.Compose([\n",
    "                                    #transforms.RandomRotation(30),\n",
    "#                                        transforms.RandomResizedCrop(224),\n",
    "                                       # transforms.RandomHorizontalFlip(),\n",
    "                                        transforms.Resize(32),\n",
    "                                        transforms.CenterCrop(32),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize([0.5,], [0.5,])]) # mean, std\n",
    " \n",
    "\n",
    "test_transforms = transforms.Compose([#transforms.Resize(255),\n",
    "                                      #transforms.CenterCrop(224),\n",
    "                                        transforms.Resize(32),\n",
    "                                        transforms.CenterCrop(32),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize([0.5,], [0.5,])]) # mean, std\n",
    "\n",
    "federated_train_loader = torch.utils.data.DataLoader( # <-- this is now a FederatedDataLoader \n",
    "    datasets.CIFAR10('/home/raoxy/data', train=True, download=True,\n",
    "                   transform=train_transforms), # <-- NEW: we distribute the dataset across all the workers, it's now a FederatedDataset\n",
    "    batch_size=200, shuffle=True, **kwargs)\n",
    "\n",
    "\n",
    "federated_test_loader = torch.utils.data.DataLoader( # <-- this is now a FederatedDataLoader \n",
    "    datasets.CIFAR10('/home/raoxy/data', train=False, download=True,\n",
    "                   transform=test_transforms), # <-- NEW: we distribute the dataset across all the workers, it's now a FederatedDataset\n",
    "    batch_size=200, shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T06:58:17.450563Z",
     "start_time": "2024-02-28T06:58:17.444939Z"
    },
    "code_folding": [
     5
    ]
   },
   "outputs": [],
   "source": [
    "class D_CNN(nn.Module):\n",
    "    # 定义模型的构造函数\n",
    "    def __init__(self):\n",
    "        # 调用父类的构造函数\n",
    "        super(D_CNN, self).__init__()\n",
    "        # 定义激活函数为Sigmoid\n",
    "        act = nn.Sigmoid\n",
    "        # 定义模型的卷积部分，包括四个卷积层和两个池化层，以及Sigmoid激活函数\n",
    "        self.body = nn.Sequential(\n",
    "            # 第一个卷积层，输入通道为3，输出通道为12，卷积核大小为5，填充为2，步长为2\n",
    "            nn.Conv2d(3, 12, kernel_size=5, padding=5//2, stride=2),\n",
    "            # 第一个激活层，使用Sigmoid函数\n",
    "            act(),\n",
    "            # 第二个卷积层，输入通道为12，输出通道为12，卷积核大小为5，填充为2，步长为2\n",
    "            nn.Conv2d(12, 12, kernel_size=5, padding=5//2, stride=2),\n",
    "            # 第二个激活层，使用Sigmoid函数\n",
    "            act(),\n",
    "            # 第三个卷积层，输入通道为12，输出通道为12，卷积核大小为5，填充为2，步长为1\n",
    "            nn.Conv2d(12, 12, kernel_size=5, padding=5//2, stride=1),\n",
    "            # 第三个激活层，使用Sigmoid函数\n",
    "            act(),\n",
    "            # 第四个卷积层，输入通道为12，输出通道为12，卷积核大小为5，填充为2，步长为1\n",
    "            nn.Conv2d(12, 12, kernel_size=5, padding=5//2, stride=1),\n",
    "            # 第四个激活层，使用Sigmoid函数\n",
    "            act(),\n",
    "        )\n",
    "        # 定义模型的全连接部分，包括一个线性层\n",
    "        self.fc = nn.Sequential(\n",
    "            # 第一个线性层，输入特征为768，输出特征为10，表示10个类别\n",
    "            nn.Linear(768, 10),\n",
    "            # 注释掉了第二个激活层和第二个线性层，可能是为了简化模型\n",
    "            #act(),\n",
    "            #nn.Linear(256, 10)\n",
    "        )\n",
    "        \n",
    "    # 定义模型的前向传播函数\n",
    "    def forward(self, x):\n",
    "        # 将输入数据通过卷积部分，得到输出\n",
    "        out = self.body(x)\n",
    "        # 将输出展平为一维向量，形状为(N, 768)，其中N是批量大小\n",
    "        feature = out.view(out.size(0), -1)\n",
    "        # 打印特征的形状，用于调试\n",
    "        #print(feature.size())\n",
    "        # 将特征通过全连接部分，得到最终的输出\n",
    "        out = self.fc(feature)\n",
    "        # 返回输出和特征\n",
    "        return out, feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T06:58:28.318619Z",
     "start_time": "2024-02-28T06:58:17.451867Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 12, 16, 16]             912\n",
      "           Sigmoid-2           [-1, 12, 16, 16]               0\n",
      "            Conv2d-3             [-1, 12, 8, 8]           3,612\n",
      "           Sigmoid-4             [-1, 12, 8, 8]               0\n",
      "            Conv2d-5             [-1, 12, 8, 8]           3,612\n",
      "           Sigmoid-6             [-1, 12, 8, 8]               0\n",
      "            Conv2d-7             [-1, 12, 8, 8]           3,612\n",
      "           Sigmoid-8             [-1, 12, 8, 8]               0\n",
      "            Linear-9                   [-1, 10]           7,690\n",
      "================================================================\n",
      "Total params: 19,438\n",
      "Trainable params: 19,438\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.08\n",
      "Params size (MB): 0.07\n",
      "Estimated Total Size (MB): 0.17\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = D_CNN().to(device)\n",
    "from torchsummary import summary\n",
    "summary(model,(3,32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T06:58:28.324022Z",
     "start_time": "2024-02-28T06:58:28.321428Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def cross_entropy_for_onehot(pred, target):\n",
    "    # 对预测值进行log_softmax操作，然后与目标值相乘，再求和，最后求平均\n",
    "    return torch.mean(torch.sum(- target * F.log_softmax(pred, dim=-1), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T06:58:28.347783Z",
     "start_time": "2024-02-28T06:58:28.325060Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "criterion =  nn.CrossEntropyLoss().to(device)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.002,momentum=0.9) # TODO momentum is not supported at the moment\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.002) # TODO momentum is not supported at the moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T06:58:28.363908Z",
     "start_time": "2024-02-28T06:58:28.349100Z"
    },
    "code_folding": [
     2
    ]
   },
   "outputs": [],
   "source": [
    "loss_test=[]\n",
    "acc_test=[]\n",
    "def test(model, device, federated_test_loader, batch_size):\n",
    "    global criterion,txt,loss_test,acc_test\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    n=0\n",
    "    with torch.no_grad():\n",
    "        for data, target in federated_test_loader:\n",
    "#             model.send(data.location) # <-- NEW: send the model in virtual workers to Trusted Aggregator\n",
    "            ##########################################################################\n",
    "#             target = target.to(device) \n",
    "#             data=compress_channel(data, 28)\n",
    "#             optimizer.zero_grad()\n",
    "#             output = model(data.to(device))\n",
    "            ##########################################################################\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output, feature_fc1_graph = model(data)\n",
    "            #########################################################################\n",
    "            n += target.shape[0]\n",
    "            loss = criterion(output, target.long())\n",
    "#             loss = F.nll_loss(output, target, reduction='sum')\n",
    "#             model.get()\n",
    "#             test_loss += loss.get() # sum up batch loss\n",
    "            test_loss += loss.item() # sum up batch loss\n",
    "            pred = output.argmax(1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            # print(\"{}\\t{}\".format(correct,target.shape))\n",
    "\n",
    "    test_loss /= len(federated_test_loader)\n",
    "    loss_test.append(test_loss)\n",
    "    acc_test.append(correct*1.0/n)\n",
    "\n",
    "    print('\\tTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        test_loss, correct, len(federated_test_loader) * batch_size,\n",
    "        100. * correct / n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T06:58:28.385375Z",
     "start_time": "2024-02-28T06:58:28.365102Z"
    }
   },
   "outputs": [],
   "source": [
    "def KSVD(channel,k):\n",
    "    from  ksvd import ApproximateKSVD\n",
    "    ksvd = ApproximateKSVD(n_components=k)\n",
    "    D=ksvd.fit(channel).components_\n",
    "    L=ksvd.transform(channel)\n",
    "    return L.dot(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T06:58:28.401044Z",
     "start_time": "2024-02-28T06:58:28.386587Z"
    }
   },
   "outputs": [],
   "source": [
    "def compress_channel(data, k,batch_size=200):\n",
    "    data = data.numpy()\n",
    "    output_data =np.zeros_like(data)\n",
    "    for i in range(len(data)):\n",
    "        channel1 = data[i, 0, :, :]\n",
    "        channel2 = data[i, 1, :, :]\n",
    "        channel3 = data[i, 2, :, :]\n",
    "        \n",
    "        output_data[i, 0, :, :] =KSVD(channel1,k)\n",
    "        output_data[i, 1, :, :] =KSVD(channel1,k)\n",
    "        output_data[i, 2, :, :] =KSVD(channel1,k)\n",
    "        \n",
    "    return torch.from_numpy(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T06:58:28.424060Z",
     "start_time": "2024-02-28T06:58:28.402168Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "loss_train=[]\n",
    "acc_train=[]\n",
    "\n",
    "def train(model, device, federated_train_loader, optimizer, epoch, batch_size):\n",
    "    global out,target,criterion,txt,loss_train,acc_train,deviation_f1_x_norm_sum,thresh,deviation_f1_x_norm,feature_fc1_graph\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    n=0\n",
    "    epsilon=50\n",
    "    for batch_idx, (data, target) in enumerate(federated_train_loader): # <-- now it is a distributed dataset\n",
    "        ##########################[KSVD分解]###############################\n",
    "        target = target.to(device) \n",
    "        data=compress_channel(data, 33)\n",
    "        data.requires_grad = True\n",
    "        out, feature_fc1_graph = model(data.to(device))\n",
    "        ################################################################\n",
    "        deviation_f1_target = torch.zeros_like(feature_fc1_graph) # 创建一个全零的张量，用于存储目标梯度\n",
    "        deviation_f1_x_norm = torch.zeros_like(feature_fc1_graph) # 创建一个全零的张量，用于存储导数的范数\n",
    "        \n",
    "        for f in range(deviation_f1_x_norm.size(1)): # 对于每个特征向量的维度\n",
    "            deviation_f1_target[:,f] = 1 # 将目标梯度的对应位置设为1\n",
    "            feature_fc1_graph.backward(deviation_f1_target, retain_graph=True) # 对特征向量进行反向传播，计算梯度\n",
    "            deviation_f1_x = data.grad.data # 获取输入的梯度\n",
    "            deviation_f1_x = deviation_f1_x.to(device) # 获取输入的梯度\n",
    "            deviation_f1_x_norm[:,f] = torch.norm(deviation_f1_x.view(deviation_f1_x.size(0), -1), dim=1)/ torch.where(feature_fc1_graph[:, f] == 0, torch.ones_like(feature_fc1_graph[:, f]), feature_fc1_graph[:, f])\n",
    "            \n",
    "#             (feature_fc1_graph.data[:,f]+0.00001) # 计算梯度的范数与特征向量的比值\n",
    "            model.zero_grad() # 清零网络的梯度\n",
    "            data.grad.data.zero_() # 清零输入的梯度\n",
    "            deviation_f1_target[:,f] = 0 # 将目标梯度的对应位置设为0\n",
    "\n",
    "        deviation_f1_x_norm_sum = deviation_f1_x_norm.sum(axis=0) # 对每个维度求和\n",
    "        thresh = np.percentile(deviation_f1_x_norm_sum.flatten().cpu().detach().numpy(), epsilon) # 根据百分位数确定阈值\n",
    "        mask = np.where(abs(deviation_f1_x_norm_sum.cpu()) < thresh, np.random.laplace(0,1e-1), 1).astype(np.float32)\n",
    "                    \n",
    "        n += target.shape[0]\n",
    "        y = criterion(out, target.long())\n",
    "        y.backward(retain_graph=True)\n",
    "        \n",
    "        dy_dx = torch.autograd.grad(y, model.parameters()) # 计算损失对网络参数的梯度\n",
    "        original_dy_dx = list((_.detach().clone() for _ in dy_dx)) # 复制梯度\n",
    "        original_dy_dx[-2] = original_dy_dx[-2]* torch.Tensor(mask).to(device) # 将梯度乘以掩码，实现剪枝\n",
    "        \n",
    "        pred = out.argmax(1, keepdim=True) # get the index of the max log-probability\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        train_loss+=y.item()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss /= len(federated_train_loader)\n",
    "    loss_train.append(train_loss)\n",
    "    acc_train.append(correct*1.0/ n)\n",
    "\n",
    "    print('Train set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(train_loss, correct, len(federated_train_loader) * batch_size,100.*correct / n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T08:14:48.803878Z",
     "start_time": "2024-02-28T06:58:28.425198Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(50):\n",
    "    train(model, device, federated_train_loader, optimizer, epoch, batch_size=200)\n",
    "    test(model, device, federated_test_loader, batch_size=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T08:14:48.806324Z",
     "start_time": "2024-02-28T08:14:48.806311Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "mpl.use('nbAgg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T08:14:48.807633Z",
     "start_time": "2024-02-28T08:14:48.807620Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame([loss_train, loss_test, acc_train, acc_test]).T\n",
    "df.columns =['loss_train', 'loss_test','acc_train','acc_test']\n",
    "# df.to_csv(\"/home/raoxy/file/{}\".format(save_name),index=False)\n",
    "mpl.style.use('seaborn-white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T08:14:48.808676Z",
     "start_time": "2024-02-28T08:14:48.808664Z"
    }
   },
   "outputs": [],
   "source": [
    "# 将DataFrame中的数据进行可视化，设置两个y轴\n",
    "ax = df[['loss_train', 'loss_test']].plot(color=['#CD0056','#F47EAB'])\n",
    "# 创建一个新的Axes对象，共享x轴\n",
    "ax2 = ax.twinx()\n",
    "# 绘制'acc_train'和'acc_test'在右侧y轴\n",
    "df[['acc_train', 'acc_test']].plot(ax=ax2, color=['#0C755F', '#A2C69B'])\n",
    "# 设置左侧y轴标签\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_xlabel('epoch')\n",
    "# 设置右侧y轴标签\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax.grid(True)\n",
    "ax2.grid(True)\n",
    "ax.legend(loc='center')\n",
    "ax2.legend(loc='center right')\n",
    "# 显示图形\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T08:14:48.810002Z",
     "start_time": "2024-02-28T08:14:48.809990Z"
    }
   },
   "outputs": [],
   "source": [
    "# 保存模型参数\n",
    "model_name=\"My_CIFAR10\"\n",
    "torch.save(model.state_dict(), \"{}.pth\".format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T08:14:48.810897Z",
     "start_time": "2024-02-28T08:14:48.810886Z"
    }
   },
   "outputs": [],
   "source": [
    "# 保存整个模型\n",
    "torch.save(model, \"{}.pt\".format(model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型评价"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T08:14:48.812262Z",
     "start_time": "2024-02-28T08:14:48.812250Z"
    }
   },
   "outputs": [],
   "source": [
    "# 导入所需的库和模块\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "\n",
    "# 定义设备，可以是 CPU 或 GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 加载MNIST数据集，并进行预处理\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='/home/raoxy/data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = [str(i) for i in range(1,101)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T08:14:48.813316Z",
     "start_time": "2024-02-28T08:14:48.813304Z"
    }
   },
   "outputs": [],
   "source": [
    "# # 定义CNN网络的结构\n",
    "class D_CNN(nn.Module):\n",
    "    # 定义模型的构造函数\n",
    "    def __init__(self):\n",
    "        # 调用父类的构造函数\n",
    "        super(D_CNN, self).__init__()\n",
    "        # 定义激活函数为Sigmoid\n",
    "        act = nn.Sigmoid\n",
    "        # 定义模型的卷积部分，包括四个卷积层和两个池化层，以及Sigmoid激活函数\n",
    "        self.body = nn.Sequential(\n",
    "            # 第一个卷积层，输入通道为3，输出通道为12，卷积核大小为5，填充为2，步长为2\n",
    "            nn.Conv2d(3, 12, kernel_size=5, padding=5//2, stride=2),\n",
    "            # 第一个激活层，使用Sigmoid函数\n",
    "            act(),\n",
    "            # 第二个卷积层，输入通道为12，输出通道为12，卷积核大小为5，填充为2，步长为2\n",
    "            nn.Conv2d(12, 12, kernel_size=5, padding=5//2, stride=2),\n",
    "            # 第二个激活层，使用Sigmoid函数\n",
    "            act(),\n",
    "            # 第三个卷积层，输入通道为12，输出通道为12，卷积核大小为5，填充为2，步长为1\n",
    "            nn.Conv2d(12, 12, kernel_size=5, padding=5//2, stride=1),\n",
    "            # 第三个激活层，使用Sigmoid函数\n",
    "            act(),\n",
    "            # 第四个卷积层，输入通道为12，输出通道为12，卷积核大小为5，填充为2，步长为1\n",
    "            nn.Conv2d(12, 12, kernel_size=5, padding=5//2, stride=1),\n",
    "            # 第四个激活层，使用Sigmoid函数\n",
    "            act(),\n",
    "        )\n",
    "        # 定义模型的全连接部分，包括一个线性层\n",
    "        self.fc = nn.Sequential(\n",
    "            # 第一个线性层，输入特征为768，输出特征为10，表示10个类别\n",
    "            nn.Linear(768, 10),\n",
    "            # 注释掉了第二个激活层和第二个线性层，可能是为了简化模型\n",
    "            #act(),\n",
    "            #nn.Linear(256, 10)\n",
    "        )\n",
    "        \n",
    "    # 定义模型的前向传播函数\n",
    "    def forward(self, x):\n",
    "        # 将输入数据通过卷积部分，得到输出\n",
    "        out = self.body(x)\n",
    "        # 将输出展平为一维向量，形状为(N, 768)，其中N是批量大小\n",
    "        feature = out.view(out.size(0), -1)\n",
    "        # 打印特征的形状，用于调试\n",
    "        #print(feature.size())\n",
    "        # 将特征通过全连接部分，得到最终的输出\n",
    "        out = self.fc(feature)\n",
    "        # 返回输出和特征\n",
    "        return out, feature\n",
    "# # 加载训练好的 .pt 文件\n",
    "model = D_CNN()\n",
    "model.load_state_dict(torch.load('/home/raoxy/iov-fl/Sotria_CIFAR10.pth', map_location=device))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T08:14:48.814231Z",
     "start_time": "2024-02-28T08:14:48.814220Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_test=[]\n",
    "acc_test=[]\n",
    "y_true=[]\n",
    "y_pred=[]\n",
    "log_softmax = nn.LogSoftmax(dim=1)\n",
    "def Pre(model, device, federated_test_loader, batch_size=200):\n",
    "    global criterion,txt,loss_test,acc_test\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    n=0\n",
    "    with torch.no_grad():\n",
    "        for data, target in federated_test_loader:\n",
    "#             model.send(data.location) # <-- NEW: send the model in virtual workers to Trusted Aggregator\n",
    "            ##########################################################################\n",
    "#             target = target.to(device) \n",
    "#             data=compress_channel(data, 28)\n",
    "#             optimizer.zero_grad()\n",
    "#             output = model(data.to(device))\n",
    "            ##########################################################################\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output, feature_fc1_graph = model(data)\n",
    "            output = log_softmax(output)\n",
    "            #########################################################################\n",
    "            n += target.shape[0]\n",
    "            loss = criterion(output, target.long())\n",
    "#             loss = F.nll_loss(output, target, reduction='sum')\n",
    "#             model.get()\n",
    "#             test_loss += loss.get() # sum up batch loss\n",
    "            test_loss += loss.item() # sum up batch loss\n",
    "            pred = output.argmax(1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "#             print(\"{}\\t{}\\t{}\".format(pred.shape,target.shape,accuracy_score(pred.cpu(), target.cpu())))\n",
    "            y_true.extend(target.cpu().numpy())\n",
    "#             print(y_true)\n",
    "            y_pred.extend(pred.squeeze().cpu().numpy())\n",
    "\n",
    "    test_loss /= len(federated_test_loader)\n",
    "    loss_test.append(test_loss)\n",
    "    acc_test.append(correct*1.0/n)\n",
    "\n",
    "    print('\\tTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        test_loss, correct, len(federated_test_loader) * batch_size,\n",
    "        100. * correct / n))\n",
    "    return y_true,y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T08:14:48.815135Z",
     "start_time": "2024-02-28T08:14:48.815124Z"
    }
   },
   "outputs": [],
   "source": [
    "y_true,y_pred=Pre(model, device, federated_test_loader, batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T08:14:48.816484Z",
     "start_time": "2024-02-28T08:14:48.816472Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score  # pip install scikit-learn\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(\"精度: \",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T08:14:48.817515Z",
     "start_time": "2024-02-28T08:14:48.817503Z"
    }
   },
   "outputs": [],
   "source": [
    "# 绘制混淆矩阵\n",
    "confusion = confusion_matrix(y_true, y_pred)\n",
    "plt.imshow(confusion, cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(10)\n",
    "plt.xticks(tick_marks, classes)\n",
    "plt.yticks(tick_marks, classes)\n",
    "plt.xlabel(\"Predicted label\")\n",
    "plt.ylabel(\"True label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T08:14:48.818551Z",
     "start_time": "2024-02-28T08:14:48.818539Z"
    }
   },
   "outputs": [],
   "source": [
    "# 计算AUC值\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(10):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_true == i, y_pred == i)\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# 绘制ROC曲线\n",
    "plt.figure()\n",
    "for i in range(10):\n",
    "    plt.plot(fpr[i], tpr[i], label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "                                   ''.format(i, roc_auc[i]))\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "785px",
    "left": "120px",
    "top": "110.525px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
