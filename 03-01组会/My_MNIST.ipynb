{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参数配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T12:07:35.151945Z",
     "start_time": "2024-02-27T12:07:34.540468Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "\n",
    "# transforms\n",
    "train_transforms = transforms.Compose([#transforms.RandomRotation(30),\n",
    "                                       # transforms.RandomResizedCrop(224),\n",
    "                                       # transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize([0.5,], [0.5,])]) # mean, std\n",
    " \n",
    "\n",
    "test_transforms = transforms.Compose([#transforms.Resize(255),\n",
    "                                      #transforms.CenterCrop(224),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.5,], [0.5,])]) # mean, std\n",
    "\n",
    "federated_train_loader = torch.utils.data.DataLoader( # <-- this is now a FederatedDataLoader \n",
    "    datasets.MNIST('/home/raoxy/data', train=True, download=True,\n",
    "                   transform=train_transforms), # <-- NEW: we distribute the dataset across all the workers, it's now a FederatedDataset\n",
    "    batch_size=200, shuffle=True, **kwargs)\n",
    "\n",
    "\n",
    "federated_test_loader = torch.utils.data.DataLoader( # <-- this is now a FederatedDataLoader \n",
    "    datasets.MNIST('/home/raoxy/data', train=False, download=True,\n",
    "                   transform=test_transforms), # <-- NEW: we distribute the dataset across all the workers, it's now a FederatedDataset\n",
    "    batch_size=200, shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T12:07:40.299620Z",
     "start_time": "2024-02-27T12:07:35.154248Z"
    },
    "code_folding": [
     5
    ]
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class D_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(D_CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        x = self.fc1(x)\n",
    "        feature = nn.functional.relu(x)\n",
    "        out= self.fc2(feature)\n",
    "#             x = nn.functional.softmax(x, dim=1)\n",
    "        return out, feature\n",
    "model = D_CNN().to(device)\n",
    "    \n",
    "optimizer = optim.SGD(model.parameters(), lr=0.002,momentum=0.9) # TODO momentum is not supported at the moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T12:07:40.303753Z",
     "start_time": "2024-02-27T12:07:40.301007Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def cross_entropy_for_onehot(pred, target):\n",
    "    print(pred.shape)\n",
    "    print(target.shape)\n",
    "    # 对预测值进行log_softmax操作，然后与目标值相乘，再求和，最后求平均\n",
    "    return torch.mean(torch.sum(- target * F.log_softmax(pred, dim=-1), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T12:07:40.319059Z",
     "start_time": "2024-02-27T12:07:40.304863Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "criterion =  nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T12:07:40.335751Z",
     "start_time": "2024-02-27T12:07:40.320264Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def compress_channel(data, k,flag=\"D\",batch_size=200):\n",
    "    if flag==\"D\":\n",
    "        from  ksvd import ApproximateKSVD\n",
    "        ksvd = ApproximateKSVD(n_components=k)\n",
    "        data = data.numpy()\n",
    "        output_D =  torch.empty(batch_size, 1,k, 28, dtype=torch.float, device=\"cpu\").numpy()\n",
    "        output_L = np.zeros_like(data)\n",
    "        for i in range(len(data)):\n",
    "            channel = data[i, 0, :, :]\n",
    "            output_D[i, 0, :, :] = ksvd.fit(channel).components_\n",
    "            L = ksvd.transform(channel)\n",
    "    #         output_L[i, 0, :, :]  = np.clip(L, 0, 255)\n",
    "        return torch.from_numpy(output_D)#torch.from_numpy(output_L)\n",
    "    elif flag==\"L\":\n",
    "        from  ksvd import ApproximateKSVD\n",
    "        ksvd = ApproximateKSVD(n_components=k)\n",
    "        data = data.numpy()\n",
    "        output_D =  torch.empty(batch_size, 1,k, 28, dtype=torch.float, device=\"cpu\").numpy()\n",
    "        output_L = np.zeros_like(data)\n",
    "        for i in range(len(data)):\n",
    "            channel = data[i, 0, :, :]\n",
    "            output_D[i, 0, :, :] = ksvd.fit(channel).components_\n",
    "            output_L[i, 0, :, :]  = ksvd.transform(channel)\n",
    "    #         output_L[i, 0, :, :]  = np.clip(L, 0, 255)\n",
    "        return torch.from_numpy(output_L)#torch.from_numpy(output_L)\n",
    "    else:\n",
    "        from  ksvd import ApproximateKSVD\n",
    "        ksvd = ApproximateKSVD(n_components=k)\n",
    "        data = data.numpy()\n",
    "        output_data =np.zeros_like(data)\n",
    "        for i in range(len(data)):\n",
    "            channel = data[i, 0, :, :]\n",
    "            output_D = ksvd.fit(channel).components_\n",
    "            output_L = ksvd.transform(channel)\n",
    "            output_data[i, 0, :, :] =output_L.dot(output_D)\n",
    "        return torch.from_numpy(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T12:07:40.357401Z",
     "start_time": "2024-02-27T12:07:40.337086Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "loss_test=[]\n",
    "acc_test=[]\n",
    "def test(model, device, federated_test_loader, batch_size):\n",
    "    global criterion,txt,loss_test,acc_test\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    n=0\n",
    "    with torch.no_grad():\n",
    "        for data, target in federated_test_loader:\n",
    "#             model.send(data.location) # <-- NEW: send the model in virtual workers to Trusted Aggregator\n",
    "            ##########################################################################\n",
    "#             target = target.to(device) \n",
    "#             data=compress_channel(data, 28)\n",
    "#             optimizer.zero_grad()\n",
    "#             output = model(data.to(device))\n",
    "            ##########################################################################\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output, feature_fc1_graph = model(data)\n",
    "            #########################################################################\n",
    "            n += target.shape[0]\n",
    "            loss = criterion(output, target.long())\n",
    "#             loss = F.nll_loss(output, target, reduction='sum')\n",
    "#             model.get()\n",
    "#             test_loss += loss.get() # sum up batch loss\n",
    "            test_loss += loss.item() # sum up batch loss\n",
    "            pred = output.argmax(1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            # print(\"{}\\t{}\".format(correct,target.shape))\n",
    "\n",
    "    test_loss /= len(federated_test_loader)\n",
    "    loss_test.append(test_loss)\n",
    "    acc_test.append(correct*1.0/n)\n",
    "\n",
    "    print('\\tTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        test_loss, correct, len(federated_test_loader) * batch_size,\n",
    "        100. * correct / n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T12:07:40.382294Z",
     "start_time": "2024-02-27T12:07:40.358473Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "loss_train=[]\n",
    "acc_train=[]\n",
    "\n",
    "def train(model, device, federated_train_loader, optimizer, epoch, batch_size):\n",
    "    global out,target,criterion,txt,loss_train,acc_train,deviation_f1_x_norm_sum,thresh,deviation_f1_x_norm,feature_fc1_graph\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    n=0\n",
    "    epsilon=50\n",
    "    for batch_idx, (data, target) in enumerate(federated_train_loader): # <-- now it is a distributed dataset\n",
    "        ##########################[KSVD分解]###############################\n",
    "        target = target.to(device) \n",
    "        data=compress_channel(data, 32,\"ALL\")\n",
    "        data.requires_grad = True\n",
    "        out, feature_fc1_graph = model(data.to(device))\n",
    "        ################################################################\n",
    "        deviation_f1_target = torch.zeros_like(feature_fc1_graph) # 创建一个全零的张量，用于存储目标梯度\n",
    "        deviation_f1_x_norm = torch.zeros_like(feature_fc1_graph) # 创建一个全零的张量，用于存储导数的范数\n",
    "        \n",
    "        for f in range(deviation_f1_x_norm.size(1)): # 对于每个特征向量的维度\n",
    "            deviation_f1_target[:,f] = 1 # 将目标梯度的对应位置设为1\n",
    "            feature_fc1_graph.backward(deviation_f1_target, retain_graph=True) # 对特征向量进行反向传播，计算梯度\n",
    "            deviation_f1_x = data.grad.data # 获取输入的梯度\n",
    "            deviation_f1_x = deviation_f1_x.to(device) # 获取输入的梯度\n",
    "            deviation_f1_x_norm[:,f] = torch.norm(deviation_f1_x.view(deviation_f1_x.size(0), -1), dim=1)/ torch.where(feature_fc1_graph[:, f] == 0, torch.ones_like(feature_fc1_graph[:, f]), feature_fc1_graph[:, f])\n",
    "            \n",
    "#             (feature_fc1_graph.data[:,f]+0.00001) # 计算梯度的范数与特征向量的比值\n",
    "            model.zero_grad() # 清零网络的梯度\n",
    "            data.grad.data.zero_() # 清零输入的梯度\n",
    "            deviation_f1_target[:,f] = 0 # 将目标梯度的对应位置设为0\n",
    "\n",
    "        deviation_f1_x_norm_sum = deviation_f1_x_norm.sum(axis=0) # 对每个维度求和\n",
    "        thresh = np.percentile(deviation_f1_x_norm_sum.flatten().cpu().detach().numpy(), epsilon) # 根据百分位数确定阈值\n",
    "        mask = np.where(abs(deviation_f1_x_norm_sum.cpu()) < thresh, np.random.laplace(0,1e-1), 1).astype(np.float32)\n",
    "                    \n",
    "        n += target.shape[0]\n",
    "        y = criterion(out, target.long())\n",
    "        y.backward(retain_graph=True)\n",
    "        \n",
    "        dy_dx = torch.autograd.grad(y, model.parameters()) # 计算损失对网络参数的梯度\n",
    "        original_dy_dx = list((_.detach().clone() for _ in dy_dx)) # 复制梯度\n",
    "        original_dy_dx[-2] = original_dy_dx[-2]* torch.Tensor(mask).to(device) # 将梯度乘以掩码，实现剪枝\n",
    "        \n",
    "        pred = out.argmax(1, keepdim=True) # get the index of the max log-probability\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        train_loss+=y.item()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss /= len(federated_train_loader)\n",
    "    loss_train.append(train_loss)\n",
    "    acc_train.append(correct*1.0/ n)\n",
    "\n",
    "    print('Train set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(train_loss, correct, len(federated_train_loader) * batch_size,100.*correct / n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-27T12:07:32.789Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: Average loss: 1.1194, Accuracy: 41943/60000 (70%)\n",
      "\tTest set: Average loss: 0.2997, Accuracy: 9115/10000 (91%)\n",
      "Train set: Average loss: 0.2724, Accuracy: 55120/60000 (92%)\n",
      "\tTest set: Average loss: 0.1611, Accuracy: 9526/10000 (95%)\n",
      "Train set: Average loss: 0.1807, Accuracy: 56746/60000 (95%)\n",
      "\tTest set: Average loss: 0.1158, Accuracy: 9629/10000 (96%)\n",
      "Train set: Average loss: 0.1429, Accuracy: 57434/60000 (96%)\n",
      "\tTest set: Average loss: 0.0837, Accuracy: 9732/10000 (97%)\n",
      "Train set: Average loss: 0.1236, Accuracy: 57756/60000 (96%)\n",
      "\tTest set: Average loss: 0.0785, Accuracy: 9746/10000 (97%)\n",
      "Train set: Average loss: 0.1106, Accuracy: 57970/60000 (97%)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(50):\n",
    "    train(model, device, federated_train_loader, optimizer, epoch, batch_size=200)\n",
    "    test(model, device, federated_test_loader, batch_size=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-27T12:07:32.800Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "mpl.use('nbAgg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-27T12:07:32.801Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame([loss_train, loss_test, acc_train, acc_test]).T\n",
    "df.columns =['loss_train', 'loss_test','acc_train','acc_test']\n",
    "# df.to_csv(\"/home/raoxy/file/{}\".format(save_name),index=False)\n",
    "mpl.style.use('seaborn-white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-27T12:07:32.802Z"
    }
   },
   "outputs": [],
   "source": [
    "# 将DataFrame中的数据进行可视化，设置两个y轴\n",
    "ax = df[['loss_train', 'loss_test']].plot(color=['#CD0056','#F47EAB'])\n",
    "# 创建一个新的Axes对象，共享x轴\n",
    "ax2 = ax.twinx()\n",
    "# 绘制'acc_train'和'acc_test'在右侧y轴\n",
    "df[['acc_train', 'acc_test']].plot(ax=ax2, color=['#0C755F', '#A2C69B'])\n",
    "# 设置左侧y轴标签\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_xlabel('epoch')\n",
    "# 设置右侧y轴标签\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax.grid(True)\n",
    "ax2.grid(True)\n",
    "ax.legend(loc='center')\n",
    "ax2.legend(loc='center right')\n",
    "# 显示图形\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-27T12:07:32.814Z"
    }
   },
   "outputs": [],
   "source": [
    "# 保存模型参数\n",
    "model_name=\"My_MNIST\"\n",
    "torch.save(model.state_dict(), \"{}.pth\".format(model_name))\n",
    "# 保存整个模型\n",
    "torch.save(model, \"{}.pt\".format(model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型评价"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-27T12:07:32.825Z"
    }
   },
   "outputs": [],
   "source": [
    "# 导入所需的库和模块\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "\n",
    "# 定义设备，可以是 CPU 或 GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 加载MNIST数据集，并进行预处理\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='/home/raoxy/data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('0', '1', '2', '3', '4',\n",
    "           '5', '6', '7', '8', '9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-27T12:07:32.826Z"
    }
   },
   "outputs": [],
   "source": [
    "# # 定义CNN网络的结构\n",
    "class D_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(D_CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        x = self.fc1(x)\n",
    "        feature = nn.functional.relu(x)\n",
    "        out= self.fc2(feature)\n",
    "#             x = nn.functional.softmax(x, dim=1)\n",
    "        return out, feature\n",
    "# # 加载训练好的 .pt 文件\n",
    "model = D_CNN()\n",
    "model.load_state_dict(torch.load('/home/raoxy/iov-fl/My_MNIST.pth', map_location=device))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-27T12:07:32.829Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_test=[]\n",
    "acc_test=[]\n",
    "y_true=[]\n",
    "y_pred=[]\n",
    "log_softmax = nn.LogSoftmax(dim=1)\n",
    "def Pre(model, device, federated_test_loader, batch_size=200):\n",
    "    global criterion,txt,loss_test,acc_test\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    n=0\n",
    "    with torch.no_grad():\n",
    "        for data, target in federated_test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output, feature_fc1_graph = model(data)\n",
    "            output = log_softmax(output)\n",
    "            #########################################################################\n",
    "            n += target.shape[0]\n",
    "            loss = criterion(output, target.long())\n",
    "            test_loss += loss.item() # sum up batch loss\n",
    "            pred = output.argmax(1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "#             print(\"{}\\t{}\\t{}\".format(pred.shape,target.shape,accuracy_score(pred.cpu(), target.cpu())))\n",
    "            y_true.extend(target.cpu().numpy())\n",
    "#             print(y_true)\n",
    "            y_pred.extend(pred.squeeze().cpu().numpy())\n",
    "\n",
    "    test_loss /= len(federated_test_loader)\n",
    "    loss_test.append(test_loss)\n",
    "    acc_test.append(correct*1.0/n)\n",
    "\n",
    "    print('\\tTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        test_loss, correct, len(federated_test_loader) * batch_size,\n",
    "        100. * correct / n))\n",
    "    return y_true,y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-27T12:07:32.831Z"
    }
   },
   "outputs": [],
   "source": [
    "y_true,y_pred=Pre(model, device, federated_test_loader, batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-27T12:07:32.832Z"
    }
   },
   "outputs": [],
   "source": [
    "# 计算精度\n",
    "from sklearn.metrics import accuracy_score  # pip install scikit-learn\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(\"精度: \",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-27T12:07:32.833Z"
    }
   },
   "outputs": [],
   "source": [
    "# 绘制混淆矩阵\n",
    "confusion = confusion_matrix(y_true, y_pred)\n",
    "plt.imshow(confusion, cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(10)\n",
    "plt.xticks(tick_marks, classes)\n",
    "plt.yticks(tick_marks, classes)\n",
    "plt.xlabel(\"Predicted label\")\n",
    "plt.ylabel(\"True label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-27T12:07:32.834Z"
    }
   },
   "outputs": [],
   "source": [
    "# 计算AUC值\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(10):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_true == i, y_pred == i)\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# 绘制ROC曲线\n",
    "plt.figure()\n",
    "for i in range(10):\n",
    "    plt.plot(fpr[i], tpr[i], label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "                                   ''.format(i, roc_auc[i]))\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "785px",
    "left": "120px",
    "top": "110.525px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
