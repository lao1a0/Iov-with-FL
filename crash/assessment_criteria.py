import torch
from torch import nn
from torch.utils.data import DataLoader
from utils import LoadData, write_result
import pandas as pd
import os
from sklearn.metrics import *  # pip install scikit-learn
import matplotlib.pyplot as plt  # pip install matplotlib
import numpy as np  # pip install numpy
from numpy import interp
from sklearn.preprocessing import label_binarize
import torchvision


class Arguments():
    def __init__(self):
        self.verify_root = r"../data/verify_yeo_224/"
        self.predicted_label_txt = '../files/predicted_label_cnn_yeo.txt'
        self.true_label_txt = '../files/true_label_cnn_yeo.txt'  # 正确标签的位置
        self.model_weight = "../model/yeo_cnn_lr0.01_epochs50.pt"
        self.flag = 'CNN'
        self.save_name = 'yeo_cnn_lr0.01_epochs50_verify'
        self.confusion_name ='yeo_cnn_lr0.01_epochs50_verify_confusion'
        self.roc_name='yeo_cnn_lr0.01_epochs50_verify_roc'

args = Arguments()


################################################################################### 生成预测的配置文件
def Create_data_dir_with_true_label(_root, save_name):
    ''':cvar保存每一张图片的地址+对应的正确的标签'''
    _list = []
    for a, b, c in os.walk(_root):
        for i in range(len(c)):
            _list.append(os.path.join(a, c[i]))
    print(_list)
    with open(save_name, 'w', encoding='UTF-8') as f:
        for _img in _list:
            f.write(_img + '\t' + str(_img.split('/')[-2]) + '\n')


Create_data_dir_with_true_label(args.verify_root, args.true_label_txt)

def Create_data_dir_with_no_label(_root, save_name):
    _list = []
    for a, b, c in os.walk(_root):
        for i in range(len(c)):
            _list.append(os.path.join(a, c[i]))
    print(_list)
    with open(save_name, 'w', encoding='UTF-8') as f:
        for _img in _list:
            f.write(_img + '\t' + "0" + '\n')


Create_data_dir_with_no_label(args.verify_root, args.predicted_label_txt)

################################################################################## 模型预测并保存结果

class CNN(nn.Module):
    def __init__(self, num_class):
        super(CNN, self).__init__()
        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=5)
        self.conv2 = nn.Conv2d(64, 32, 5, padding=1)  # 输入通道数为64，输出通道数为64，卷积核大小为3x3，步长为1，填充为1
        self.pool1 = nn.MaxPool2d(2)  # 最大池化层，池化核大小为2x2
        self.gap = nn.AdaptiveAvgPool2d(5)  # 全局平均池化层
        self.fc1 = nn.Linear(32, num_class)  # 全连接层 ，输入特征维度位256 ，输出特征维度位num_class
        self.relu = nn.ReLU()  # 激活函数
        self.dropout = nn.Dropout(p=0.5)  # 随机失活层
        self.softmax = nn.LogSoftmax(dim=1)

    def forward(self, x):
        x = self.relu(self.conv1(x))
        x = self.gap(x)
        x = self.dropout(x)
        x = self.relu(self.conv2(x))
        x = self.pool1(x)
        x = x.view(x.shape[0], -1)  # torch.Size([128, 32])
        x = self.softmax(self.fc1(x))
        return x

model = CNN(5)

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using {device} device")
model_loc = args.model_weight
model_dict = torch.load(model_loc)
model.load_state_dict(model_dict)
model = model.to(device)

def eval(dataloader, model):
    label_list = []
    likelihood_list = []
    pred_list = []
    model.eval()
    with torch.no_grad():
        # 加载数据加载器，得到里面的X（图片数据）和y(真实标签）
        for idx, (X, y) in enumerate(dataloader):
            # 将数据转到GPU
            X = X.cuda()
            # 将图片传入到模型当中就，得到预测的值pred
            pred = model(X)
            pred_softmax = torch.softmax(pred,1).cpu().numpy()
            # 获取可能性最大的标签
            label = torch.softmax(pred,1).cpu().numpy().argmax()
            label_list.append(label)
            # 获取可能性最大的值（即概率）
            likelihood = torch.softmax(pred,1).cpu().numpy().max()
            likelihood_list.append(likelihood)
            pred_list.append(pred_softmax.tolist()[0])

        return label_list,likelihood_list, pred_list


_dataloader = DataLoader(dataset=torchvision.datasets.ImageFolder(root=args.verify_root,
                                     transform=torchvision.transforms.ToTensor()), num_workers=2, pin_memory=True, batch_size=1)
label_list, likelihood_list, pred = eval(_dataloader, model)
print(label_list)
print(likelihood_list)
# 将输出保存到exel中，方便后续分析
label_names = ['R', 'RPM',  'gear', 'DoS', 'Fuzzy']
df = pd.DataFrame(data=pred,columns=label_names)
df.to_csv('../files/{}.csv'.format(args.save_name), encoding='utf-8',index=False)

# [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]
# [0.76114714, 0.76245546, 0.7412751, 0.75783736, 0.9342735, 0.75998443, 0.75499356, 0.76802474, 0.7635678, 0.7410445, 0.763712, 0.9350285, 0.763712, 0.76321185, 0.7641921, 0.763712, 0.7589567, 0.7589567, 0.7447997, 0.75777763, 0.7660359, 0.7686732, 0.7463815, 0.7167878, 0.76422924, 0.75394166, 0.7764611, 0.76039207, 0.7786772, 0.77214366, 0.9291841, 0.69624764, 0.771059, 0.77214366, 0.77214366, 0.77214366, 0.7677712, 0.7467128, 0.7677712, 0.7677712, 0.7677712, 0.73055696, 0.7435639, 0.745165, 0.76843446, 0.9490644, 0.7324353, 0.9281553, 0.72088134, 0.71410584, 0.7252369, 0.7314522, 0.9297773, 0.6878296, 0.70566946, 0.70566946, 0.70566946, 0.70566946, 0.7086011, 0.7238539, 0.7207093, 0.7207093, 0.9287435, 0.7130284, 0.7211543, 0.7282566, 0.93098813, 0.699613, 0.7165488, 0.71563923, 0.71408534, 0.7064793, 0.91426915, 0.6952332, 0.6952332, 0.7105599, 0.7100831, 0.9246717, 0.74141836, 0.68969667, 0.7282388, 0.69560003, 0.7234406, 0.6760674, 0.69197583, 0.7021712, 0.71164554, 0.7164437, 0.7165622, 0.68603104, 0.71137905, 0.66804004, 0.7029589, 0.7062273, 0.9370053, 0.6964332, 0.68008345, 0.6954003, 0.65501887, 0.66766846, 0.70266944, 0.6812536, 0.70471275, 0.70700115, 0.68467045, 0.6616385, 0.73528695, 0.7004035, 0.6954782, 0.678463, 0.72766614, 0.68218017, 0.71387523, 0.62940186, 0.6891374, 0.9369229, 0.75605226, 0.6178175, 0.6121719, 0.6121719, 0.93314713, 0.91786605, 0.9437682, 0.9331137, 0.94020164, 0.92907965, 0.9240878, 0.95252794, 0.94743687, 0.9435185, 0.95136267, 0.95198035, 0.95177317, 0.95332706, 0.9338852, 0.9579021, 0.9541029, 0.9487319, 0.95327234, 0.9530113, 0.92537135, 0.94508827, 0.9572481, 0.95527554, 0.9598648, 0.95879877, 0.9564942, 0.956661, 0.9623045, 0.9382072, 0.96610516, 0.94428545, 0.9641759, 0.9280363, 0.9396254, 0.96405166, 0.95908564, 0.95306486, 0.96197736, 0.9581125, 0.95751905, 0.9600864, 0.9598008, 0.9592914, 0.96547145, 0.96201146, 0.9582461, 0.9562227, 0.96370745, 0.9050956, 0.9606445, 0.86577016, 0.8116306, 0.76773787, 0.7645241, 0.76361865, 0.929193, 0.7552677, 0.7587783, 0.75839674, 0.76476014, 0.7605805, 0.7605805, 0.75699645, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.9970482, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99776995, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.99138415, 0.96203196, 0.98100513, 0.97641057, 0.97954476, 0.976769, 0.9798196, 0.97915065, 0.9783572, 0.97932863, 0.9802069, 0.98116875, 0.97621346, 0.97760606, 0.9743404, 0.9820279, 0.9694119, 0.978693, 0.9791007, 0.98068684, 0.9774595, 0.98037416, 0.97903043, 0.9788152, 0.98133963, 0.97434574, 0.97993267, 0.9706801, 0.9816991, 0.9746181, 0.9791372, 0.9789746, 0.9779613, 0.97866863, 0.98115516, 0.9805375, 0.9791712, 0.9777142, 0.9780593, 0.97840595, 0.9765902, 0.9747074, 0.9730215, 0.97012603, 0.9711704, 0.97244215, 0.97896796, 0.9721816, 0.9787083, 0.96982837, 0.9806771, 0.9713696, 0.9781697, 0.982898, 0.9767303, 0.9744932, 0.9789909, 0.977403, 0.9735238, 0.9790036, 0.97933424, 0.97291577, 0.97840655, 0.98129994, 0.9748316, 0.97938454, 0.9775947, 0.97926027, 0.9757161, 0.97262067, 0.9816774, 0.97918826, 0.9806358, 0.97818553, 0.97728354, 0.9683954, 0.97714764, 0.96628946, 0.97851765, 0.97994, 0.97797704, 0.9703972, 0.98482925, 0.97855467, 0.9778425, 0.9798206, 0.9806507, 0.9730419, 0.97564346, 0.980572, 0.9800071, 0.9646069, 0.9763415, 0.97775537, 0.9785064, 0.98083043, 0.98039186, 0.97485596, 0.9786719, 0.9785801, 0.9800791, 0.9726643, 0.98028505, 0.9715916, 0.97440785, 0.975869, 0.9789975, 0.9799982, 0.9742286, 0.9786078, 0.98046917, 0.97585654, 0.9793773, 0.97215366, 0.97863215, 0.972691, 0.9738168, 0.97925586, 0.98191595, 0.982236, 0.9751796, 0.9745565, 0.96930593, 0.97552377, 0.97375566, 0.98037016, 0.97200596, 0.97489697, 0.9803239, 0.9777974, 0.9724262, 0.9786332, 0.96944886, 0.9691758, 0.978566, 0.9724998, 0.97486746, 0.9732337, 0.98340607, 0.9781935, 0.97635114, 0.9775433, 0.9731678, 0.9749767, 0.9771762, 0.9735501, 0.97899354, 0.9759434, 0.9703977, 0.979039, 0.97156477, 0.97953886, 0.9773513, 0.97295177, 0.98002785, 0.9746693, 0.96879345, 0.98044246, 0.9785402, 0.9719311, 0.97905743, 0.9699832, 0.9762082, 0.97911227, 0.95655537, 0.9790036, 0.9780943, 0.97783047, 0.9683944, 0.9798608, 0.97781026, 0.97134554, 0.97498804, 0.9805233, 0.9726255, 0.9803197, 0.9828771, 0.9734971, 0.9746653, 0.98022527, 0.97432816, 0.9814317, 0.9799185, 0.9824851, 0.98261905, 0.9813399, 0.9628953, 0.9779348, 0.9795828, 0.9822989]