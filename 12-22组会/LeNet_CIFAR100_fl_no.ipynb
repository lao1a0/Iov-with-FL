{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参数配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import syft as sy\n",
    "# import copy\n",
    "# hook = sy.TorchHook(torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create a couple workers\n",
    "# bob = sy.VirtualWorker(hook, id=\"bob\")\n",
    "# alice = sy.VirtualWorker(hook, id=\"alice\")\n",
    "# secure_worker_a = sy.VirtualWorker(hook, id=\"secure_worker_a\")\n",
    "# secure_worker_b = sy.VirtualWorker(hook, id=\"secure_worker_b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforms\n",
    "train_transforms = transforms.Compose([#transforms.RandomRotation(30),\n",
    "                                       # transforms.RandomResizedCrop(224),\n",
    "                                       # transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize([0.5,], [0.5,])]) # mean, std\n",
    " \n",
    "\n",
    "test_transforms = transforms.Compose([#transforms.Resize(255),\n",
    "                                      #transforms.CenterCrop(224),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.5,], [0.5,])]) # mean, std\n",
    "\n",
    "\n",
    "# choose the training and test datasets\n",
    "\n",
    "# federated_train_loader = sy.FederatedDataLoader( # <-- this is now a FederatedDataLoader \n",
    "#     datasets.CIFAR100('/home/zhaojia-raoxy/data', train=True, download=True,\n",
    "#                    transform=train_transforms)\n",
    "#     .federate((bob, alice)), # <-- NEW: we distribute the dataset across all the workers, it's now a FederatedDataset\n",
    "#     batch_size=20, shuffle=True, **kwargs)\n",
    "\n",
    "\n",
    "# federated_test_loader = sy.FederatedDataLoader( # <-- this is now a FederatedDataLoader \n",
    "#     datasets.CIFAR100('/home/zhaojia-raoxy/data', train=False, download=True,\n",
    "#                    transform=test_transforms)\n",
    "#     .federate((secure_worker_a, secure_worker_b)), # <-- NEW: we distribute the dataset across all the workers, it's now a FederatedDataset\n",
    "#     batch_size=20, shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "federated_train_loader = torch.utils.data.DataLoader( # <-- this is now a FederatedDataLoader \n",
    "    datasets.CIFAR10('/home/raoxy/data', train=True, download=True,\n",
    "                   transform=train_transforms), # <-- NEW: we distribute the dataset across all the workers, it's now a FederatedDataset\n",
    "    batch_size=200, shuffle=True, **kwargs)\n",
    "\n",
    "\n",
    "federated_test_loader = torch.utils.data.DataLoader( # <-- this is now a FederatedDataLoader \n",
    "    datasets.CIFAR10('/home/raoxy/data', train=False, download=True,\n",
    "                   transform=test_transforms), # <-- NEW: we distribute the dataset across all the workers, it's now a FederatedDataset\n",
    "    batch_size=200, shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self, channel=3, hideen=768, num_classes=10):\n",
    "        super(LeNet, self).__init__()\n",
    "        act = nn.Sigmoid\n",
    "        self.body = nn.Sequential(\n",
    "            nn.Conv2d(channel, 12, kernel_size=5, padding=5 // 2, stride=2),\n",
    "            act(),\n",
    "            nn.Conv2d(12, 12, kernel_size=5, padding=5 // 2, stride=2),\n",
    "            act(),\n",
    "            nn.Conv2d(12, 12, kernel_size=5, padding=5 // 2, stride=1),\n",
    "            act(),\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hideen, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.body(x)\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "    \n",
    "model = LeNet(channel=3, hideen=768, num_classes=10).to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.03) # TODO momentum is not supported at the moment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_test=[]\n",
    "acc_test=[]\n",
    "def test(model, device, federated_test_loader, batch_size): \n",
    "    global criterion,txt,loss_test,acc_test\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    n=0\n",
    "    with torch.no_grad():\n",
    "        for data, target in federated_test_loader:\n",
    "#             model.send(data.location) # <-- NEW: send the model in virtual workers to Trusted Aggregator\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            n += target.shape[0]\n",
    "            loss = criterion(output, target.long())\n",
    "#             loss = F.nll_loss(output, target, reduction='sum')\n",
    "#             model.get()\n",
    "#             test_loss += loss.get() # sum up batch loss\n",
    "            test_loss += loss.item() # sum up batch loss\n",
    "            pred = output.argmax(1, keepdim=True) # get the index of the max log-probability \n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(federated_test_loader)\n",
    "    loss_test.append(test_loss)\n",
    "    acc_test.append(correct*1.0/n)\n",
    "\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        test_loss, correct, len(federated_test_loader) * batch_size,\n",
    "        100. * correct / len(federated_test_loader) / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "loss_train=[]\n",
    "acc_train=[]\n",
    "def train(model, device, federated_train_loader, optimizer, epoch, batch_size):\n",
    "    global criterion,txt,loss_train,acc_train\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    n=0\n",
    "    for batch_idx, (data, target) in enumerate(federated_train_loader): # <-- now it is a distributed dataset\n",
    "#         model.send(data.location) # <-- NEW: send the model to the right location\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        n += target.shape[0]\n",
    "        loss = criterion(output, target.long())\n",
    "#         loss = F.cross_entropy(output, target.long())\n",
    "#         loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#         model.get() # <-- NEW: get the model back\n",
    "        pred = output.argmax(1, keepdim=True) # get the index of the max log-probability \n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        train_loss+=loss.item()\n",
    "\n",
    "    train_loss /= len(federated_train_loader)\n",
    "    loss_train.append(train_loss)\n",
    "    acc_train.append(correct*1.0/ n)\n",
    "    \n",
    "    print('Train set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "    train_loss, correct, len(federated_train_loader) * batch_size,\n",
    "     100.*correct / len(federated_train_loader) / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for epoch in range(5):\n",
    "    train(model, device, federated_train_loader, optimizer, epoch, batch_size=200)\n",
    "    test(model, device, federated_test_loader, batch_size=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 结果保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "save_name='LeNet-no-fed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型的权重\n",
    "torch.save(model.state_dict(), \"/home/zhaojia-raoxy/model/out/{}.pt\".format(save_name))\n",
    "print(\"保存文件：\",\"/home/zhaojia-raoxy/model/out/{}.pt\".format(save_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存整个模型\n",
    "torch.save(model, \"/home/zhaojia-raoxy/model/out/{}.h5\".format(save_name))\n",
    "print(\"保存文件：\",\"/home/zhaojia-raoxy/model/out/{}.h5\".format(save_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "mpl.use('nbAgg')\n",
    "mpl.style.use('seaborn-darkgrid')\n",
    "import numpy as np\n",
    "\n",
    "def plotP(test_loss, train_loss, train_acc_list, test_acc_list):\n",
    "    plt.figure(figsize=(6, 8))\n",
    "    \n",
    "    x = list(range(len(train_loss)))\n",
    "    y = list(range(len(test_acc_list)))\n",
    "    \n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(x, train_loss, label=\"train_loss\")\n",
    "    plt.plot(x, test_loss, label=\"test_loss\")\n",
    "\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.legend()\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(y, train_acc_list, label=\"train_acc\")\n",
    "    plt.plot(y, test_acc_list, label=\"test_acc\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"acc\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plotP(loss_train, loss_test, acc_train, acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:pysyft]",
   "language": "python",
   "name": "conda-env-pysyft-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "785px",
    "left": "120px",
    "top": "110.525px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
