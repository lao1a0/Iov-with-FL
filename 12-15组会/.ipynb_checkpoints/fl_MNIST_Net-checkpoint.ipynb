{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Learning Practice on MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import syft as sy\n",
    "import copy\n",
    "hook = sy.TorchHook(torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a couple workers\n",
    "bob = sy.VirtualWorker(hook, id=\"bob\")\n",
    "alice = sy.VirtualWorker(hook, id=\"alice\")\n",
    "secure_worker_a = sy.VirtualWorker(hook, id=\"secure_worker_a\")\n",
    "secure_worker_b = sy.VirtualWorker(hook, id=\"secure_worker_b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforms\n",
    "train_transforms = transforms.Compose([#transforms.RandomRotation(30),\n",
    "                                       # transforms.RandomResizedCrop(224),\n",
    "                                       # transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize([0.5,], [0.5,])]) # mean, std\n",
    " \n",
    "\n",
    "test_transforms = transforms.Compose([#transforms.Resize(255),\n",
    "                                      #transforms.CenterCrop(224),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.5,], [0.5,])]) # mean, std\n",
    "\n",
    "\n",
    "# choose the training and test datasets\n",
    "\n",
    "federated_train_loader = sy.FederatedDataLoader( # <-- this is now a FederatedDataLoader \n",
    "    datasets.MNIST('/home/zhaojia-raoxy/data', train=True, download=True,\n",
    "                   transform=train_transforms)\n",
    "    .federate((bob, alice)), # <-- NEW: we distribute the dataset across all the workers, it's now a FederatedDataset\n",
    "    batch_size=20, shuffle=True, **kwargs)\n",
    "\n",
    "\n",
    "federated_test_loader = sy.FederatedDataLoader( # <-- this is now a FederatedDataLoader \n",
    "    datasets.MNIST('/home/zhaojia-raoxy/data', train=False, download=True,\n",
    "                   transform=test_transforms)\n",
    "    .federate((secure_worker_a, secure_worker_b)), # <-- NEW: we distribute the dataset across all the workers, it's now a FederatedDataset\n",
    "    batch_size=20, shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练模型\n",
    "## Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "       # convolutional layer (sees 28x28x1 image tensor)\n",
    "        self.conv1 = nn.Conv2d(1, 4, 3, padding=1)\n",
    "        # convolutional layer (sees 14x14x4 tensor after MaxPool)\n",
    "        self.conv2 = nn.Conv2d(4, 16, 3, padding=1)\n",
    "        # max pooling layer\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        # linear layer (7 * 7 * 16)\n",
    "        self.fc1 = nn.Linear(7 * 7 * 16, 512)\n",
    "        # linear layer (512 -> 10)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "        # dropout layer (p=0.20)\n",
    "        self.dropout = nn.Dropout(0.20)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # add sequence of convolutional and max pooling layers\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        # flatten image input\n",
    "        x = x.view(-1, 7 * 7 * 16)\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add 1st hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add 2nd hidden layer, with relu activation function\n",
    "        x = self.fc2(x)\n",
    "        # LogSoftMax\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "model = Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.03) # TODO momentum is not supported at the moment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CNN(nn.Module):\n",
    "#     def __init__(self, num_class):\n",
    "#         super(CNN, self).__init__()\n",
    "#         self.block1 = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels=3, out_channels=64, kernel_size=5),\n",
    "#             nn.ReLU(),\n",
    "#             nn.AdaptiveAvgPool2d(5),\n",
    "#             nn.Dropout(p=0.5)\n",
    "\n",
    "#         )\n",
    "#         self.block2 = nn.Sequential(\n",
    "#             nn.Conv2d(64, 32, 5, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(2),\n",
    "#         )\n",
    "#         self.block3 = nn.Sequential(\n",
    "#             nn.Linear(32, num_class),\n",
    "#             nn.LogSoftmax(dim=1)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.block1(x)\n",
    "#         x = self.block2(x)\n",
    "#         x = x.view(x.shape[0], -1)  # torch.Size([128, 32])\n",
    "#         x = self.block3(x)\n",
    "#         return x\n",
    "    \n",
    "# model = CNN(100).to(device)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.03) # TODO momentum is not supported at the moment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Arguments():\n",
    "#     def __init__(self):\n",
    "#         self.batch_size = 128\n",
    "#         self.epochs = 50\n",
    "#         self.lr = 0.02\n",
    "#         self.num_class = 5\n",
    "#         self.save_name = 'yeo_alexnet_lr0.01_epoch50'\n",
    "#         self.data_train = '/home/zhaojia-raoxy/data/train_yeo'\n",
    "#         self.data_test = '/home/zhaojia-raoxy/data/test_yeo'\n",
    "#         self.model_name = '/home/zhaojia-raoxy/model/resnet18-5c106cde.pth'\n",
    "#         self.save_name = 'yeo_resnet18_lr0.01_epoch50'\n",
    "#\n",
    "# args = Arguments()\n",
    "#\n",
    "# def ResNet_s(args):\n",
    "#     ''':cvar\n",
    "#     返回修改好的模型，和冻结好的参数\n",
    "#     '''\n",
    "#     from torchvision.models import resnet18, resnet34, resnet50, resnet101, resnet152  # ResNet系列\n",
    "#     pretrain_model = resnet18(pretrained=False)\n",
    "#     pretrain_model.fc = nn.Linear(pretrain_model.fc.in_features, 100)  # 将全连接层改为自己想要的分类输出\n",
    "#     pretrained_dict = torch.load(args.model_name)\n",
    "#\n",
    "#     pretrained_dict.pop('fc.weight')\n",
    "#     pretrained_dict.pop('fc.bias')\n",
    "#\n",
    "#     model_dict = pretrain_model.state_dict()\n",
    "#     pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "#\n",
    "#     model_dict.update(pretrained_dict)  # 模型参数列表进行参数更新，加载参数\n",
    "#     pretrain_model.load_state_dict(model_dict)  # 将满足条件的参数的 requires_grad 属性设置为False\n",
    "#\n",
    "# #     for name, value in pretrain_model.named_parameters():\n",
    "# #         if (name != 'fc.weight') and (name != 'fc.bias'):\n",
    "# #             value.requires_grad = False\n",
    "#     params_conv = filter(lambda p: p.requires_grad, pretrain_model.parameters())  # 要更新的参数在parms_conv当中\n",
    "#     return pretrain_model, params_conv\n",
    "#\n",
    "# model, params_conv = ResNet_s(args)\n",
    "# model=model.to(device)\n",
    "# optimizer = optim.SGD(params_conv, lr=0.03) # TODO momentum is not supported at the moment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, federated_train_loader, optimizer, epoch, batch_size):\n",
    "    model.train()\n",
    "   \n",
    "    for batch_idx, (data, target) in enumerate(federated_train_loader): # <-- now it is a distributed dataset\n",
    "        model.send(data.location) # <-- NEW: send the model to the right location\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target.long())\n",
    "#         loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        model.get() # <-- NEW: get the model back\n",
    "        \n",
    "        if batch_idx % 2000 == 0:\n",
    "            loss = loss.get() # <-- NEW: get the loss\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * batch_size, len(federated_train_loader) * batch_size,\n",
    "                100. * batch_idx / len(federated_train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, federated_test_loader, batch_size): \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in federated_test_loader:\n",
    "            model.send(data.location) # <-- NEW: send the model in virtual workers to Trusted Aggregator\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = F.nll_loss(output, target, reduction='sum')\n",
    "            model.get()\n",
    "            test_loss += loss.get() # sum up batch loss\n",
    "            pred = output.argmax(1, keepdim=True) # get the index of the max log-probability \n",
    "            correct += pred.eq(target.view_as(pred)).sum().get()\n",
    "    test_loss /= len(federated_test_loader)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(federated_test_loader) * batch_size,\n",
    "        100. * correct / len(federated_test_loader) / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for epoch in range(50):\n",
    "    train(model, device, federated_train_loader, optimizer, epoch, batch_size=20)\n",
    "    test(model, device, federated_test_loader, batch_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_name='LeNet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型的权重\n",
    "torch.save(model.state_dict(), \"/home/zhaojia-raoxy/model/{}.pt\".format(save_name))\n",
    "print(\"保存文件：\",\"/home/zhaojia-raoxy/model/{}.pt\".format(save_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存整个模型\n",
    "torch.save(model, \"/home/zhaojia-raoxy/model/{}.h5\".format(save_name))\n",
    "print(\"保存文件：\",\"/home/zhaojia-raoxy/model/{}.h5\".format(save_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt='''\n",
    "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.288246\n",
    "Train Epoch: 0 [40000/60000 (67%)]\tLoss: 0.015846\n",
    "\n",
    "Test set: Average loss: 1.4238, Accuracy: 9767/10000 (98%)\n",
    "\n",
    "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.022734\n",
    "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 0.005947\n",
    "\n",
    "Test set: Average loss: 1.0229, Accuracy: 9843/10000 (98%)\n",
    "\n",
    "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.119699\n",
    "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 0.004264\n",
    "\n",
    "Test set: Average loss: 0.7169, Accuracy: 9880/10000 (99%)\n",
    "\n",
    "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.015286\n",
    "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 0.119393\n",
    "\n",
    "Test set: Average loss: 0.6742, Accuracy: 9880/10000 (99%)\n",
    "\n",
    "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.016501\n",
    "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 0.010696\n",
    "\n",
    "Test set: Average loss: 0.5900, Accuracy: 9900/10000 (99%)\n",
    "\n",
    "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.002065\n",
    "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 0.003014\n",
    "\n",
    "Test set: Average loss: 0.6510, Accuracy: 9891/10000 (99%)\n",
    "\n",
    "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.006424\n",
    "Train Epoch: 6 [40000/60000 (67%)]\tLoss: 0.016002\n",
    "\n",
    "Test set: Average loss: 0.6109, Accuracy: 9898/10000 (99%)\n",
    "\n",
    "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.001420\n",
    "Train Epoch: 7 [40000/60000 (67%)]\tLoss: 0.063781\n",
    "\n",
    "Test set: Average loss: 0.6160, Accuracy: 9893/10000 (99%)\n",
    "\n",
    "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.077834\n",
    "Train Epoch: 8 [40000/60000 (67%)]\tLoss: 0.001377\n",
    "\n",
    "Test set: Average loss: 0.5433, Accuracy: 9912/10000 (99%)\n",
    "\n",
    "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.050094\n",
    "Train Epoch: 9 [40000/60000 (67%)]\tLoss: 0.005410\n",
    "\n",
    "Test set: Average loss: 0.4760, Accuracy: 9922/10000 (99%)\n",
    "\n",
    "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.001634\n",
    "Train Epoch: 10 [40000/60000 (67%)]\tLoss: 0.201833\n",
    "\n",
    "Test set: Average loss: 0.5149, Accuracy: 9920/10000 (99%)\n",
    "\n",
    "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.020641\n",
    "Train Epoch: 11 [40000/60000 (67%)]\tLoss: 0.000623\n",
    "\n",
    "Test set: Average loss: 0.4756, Accuracy: 9928/10000 (99%)\n",
    "\n",
    "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.000782\n",
    "Train Epoch: 12 [40000/60000 (67%)]\tLoss: 0.000656\n",
    "\n",
    "Test set: Average loss: 0.4846, Accuracy: 9919/10000 (99%)\n",
    "\n",
    "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.014043\n",
    "Train Epoch: 13 [40000/60000 (67%)]\tLoss: 0.030818\n",
    "\n",
    "Test set: Average loss: 0.5139, Accuracy: 9916/10000 (99%)\n",
    "\n",
    "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.019904\n",
    "Train Epoch: 14 [40000/60000 (67%)]\tLoss: 0.109154\n",
    "\n",
    "Test set: Average loss: 0.5153, Accuracy: 9919/10000 (99%)\n",
    "\n",
    "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.000070\n",
    "Train Epoch: 15 [40000/60000 (67%)]\tLoss: 0.002936\n",
    "\n",
    "Test set: Average loss: 0.4601, Accuracy: 9926/10000 (99%)\n",
    "\n",
    "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.000333\n",
    "Train Epoch: 16 [40000/60000 (67%)]\tLoss: 0.000356\n",
    "\n",
    "Test set: Average loss: 0.4792, Accuracy: 9924/10000 (99%)\n",
    "\n",
    "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.000499\n",
    "Train Epoch: 17 [40000/60000 (67%)]\tLoss: 0.002755\n",
    "\n",
    "Test set: Average loss: 0.4857, Accuracy: 9931/10000 (99%)\n",
    "\n",
    "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.001885\n",
    "Train Epoch: 18 [40000/60000 (67%)]\tLoss: 0.002185\n",
    "\n",
    "Test set: Average loss: 0.5394, Accuracy: 9912/10000 (99%)\n",
    "\n",
    "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.013282\n",
    "Train Epoch: 19 [40000/60000 (67%)]\tLoss: 0.000798\n",
    "\n",
    "Test set: Average loss: 0.4650, Accuracy: 9925/10000 (99%)\n",
    "\n",
    "Train Epoch: 20 [0/60000 (0%)]\tLoss: 0.005551\n",
    "Train Epoch: 20 [40000/60000 (67%)]\tLoss: 0.000429\n",
    "\n",
    "Test set: Average loss: 0.4870, Accuracy: 9928/10000 (99%)\n",
    "\n",
    "Train Epoch: 21 [0/60000 (0%)]\tLoss: 0.017526\n",
    "Train Epoch: 21 [40000/60000 (67%)]\tLoss: 0.001950\n",
    "\n",
    "Test set: Average loss: 0.4663, Accuracy: 9930/10000 (99%)\n",
    "\n",
    "Train Epoch: 22 [0/60000 (0%)]\tLoss: 0.003154\n",
    "Train Epoch: 22 [40000/60000 (67%)]\tLoss: 0.001649\n",
    "\n",
    "Test set: Average loss: 0.4957, Accuracy: 9922/10000 (99%)\n",
    "\n",
    "Train Epoch: 23 [0/60000 (0%)]\tLoss: 0.000347\n",
    "Train Epoch: 23 [40000/60000 (67%)]\tLoss: 0.000088\n",
    "\n",
    "Test set: Average loss: 0.5050, Accuracy: 9926/10000 (99%)\n",
    "\n",
    "Train Epoch: 24 [0/60000 (0%)]\tLoss: 0.006175\n",
    "Train Epoch: 24 [40000/60000 (67%)]\tLoss: 0.185872\n",
    "\n",
    "Test set: Average loss: 0.4338, Accuracy: 9934/10000 (99%)\n",
    "\n",
    "Train Epoch: 25 [0/60000 (0%)]\tLoss: 0.000254\n",
    "Train Epoch: 25 [40000/60000 (67%)]\tLoss: 0.000881\n",
    "\n",
    "Test set: Average loss: 0.4850, Accuracy: 9924/10000 (99%)\n",
    "\n",
    "Train Epoch: 26 [0/60000 (0%)]\tLoss: 0.000533\n",
    "Train Epoch: 26 [40000/60000 (67%)]\tLoss: 0.012661\n",
    "\n",
    "Test set: Average loss: 0.4743, Accuracy: 9926/10000 (99%)\n",
    "\n",
    "Train Epoch: 27 [0/60000 (0%)]\tLoss: 0.000135\n",
    "Train Epoch: 27 [40000/60000 (67%)]\tLoss: 0.000505\n",
    "\n",
    "Test set: Average loss: 0.4599, Accuracy: 9930/10000 (99%)\n",
    "\n",
    "Train Epoch: 28 [0/60000 (0%)]\tLoss: 0.001953\n",
    "Train Epoch: 28 [40000/60000 (67%)]\tLoss: 0.000432\n",
    "\n",
    "Test set: Average loss: 0.4634, Accuracy: 9925/10000 (99%)\n",
    "\n",
    "Train Epoch: 29 [0/60000 (0%)]\tLoss: 0.010358\n",
    "Train Epoch: 29 [40000/60000 (67%)]\tLoss: 0.000214\n",
    "\n",
    "Test set: Average loss: 0.5432, Accuracy: 9923/10000 (99%)\n",
    "\n",
    "Train Epoch: 30 [0/60000 (0%)]\tLoss: 0.002619\n",
    "Train Epoch: 30 [40000/60000 (67%)]\tLoss: 0.000810\n",
    "\n",
    "Test set: Average loss: 0.5052, Accuracy: 9931/10000 (99%)\n",
    "\n",
    "Train Epoch: 31 [0/60000 (0%)]\tLoss: 0.000172\n",
    "Train Epoch: 31 [40000/60000 (67%)]\tLoss: 0.000311\n",
    "\n",
    "Test set: Average loss: 0.5174, Accuracy: 9924/10000 (99%)\n",
    "\n",
    "Train Epoch: 32 [0/60000 (0%)]\tLoss: 0.000064\n",
    "Train Epoch: 32 [40000/60000 (67%)]\tLoss: 0.000006\n",
    "\n",
    "Test set: Average loss: 0.5177, Accuracy: 9922/10000 (99%)\n",
    "\n",
    "Train Epoch: 33 [0/60000 (0%)]\tLoss: 0.000093\n",
    "Train Epoch: 33 [40000/60000 (67%)]\tLoss: 0.003240\n",
    "\n",
    "Test set: Average loss: 0.4484, Accuracy: 9930/10000 (99%)\n",
    "\n",
    "Train Epoch: 34 [0/60000 (0%)]\tLoss: 0.003127\n",
    "Train Epoch: 34 [40000/60000 (67%)]\tLoss: 0.004683\n",
    "\n",
    "Test set: Average loss: 0.5331, Accuracy: 9925/10000 (99%)\n",
    "\n",
    "Train Epoch: 35 [0/60000 (0%)]\tLoss: 0.000037\n",
    "Train Epoch: 35 [40000/60000 (67%)]\tLoss: 0.000266\n",
    "\n",
    "Test set: Average loss: 0.5182, Accuracy: 9923/10000 (99%)\n",
    "\n",
    "Train Epoch: 36 [0/60000 (0%)]\tLoss: 0.000347\n",
    "Train Epoch: 36 [40000/60000 (67%)]\tLoss: 0.000092\n",
    "\n",
    "Test set: Average loss: 0.6141, Accuracy: 9918/10000 (99%)\n",
    "\n",
    "Train Epoch: 37 [0/60000 (0%)]\tLoss: 0.006742\n",
    "Train Epoch: 37 [40000/60000 (67%)]\tLoss: 0.033185\n",
    "\n",
    "Test set: Average loss: 0.4996, Accuracy: 9926/10000 (99%)\n",
    "\n",
    "Train Epoch: 38 [0/60000 (0%)]\tLoss: 0.000365\n",
    "Train Epoch: 38 [40000/60000 (67%)]\tLoss: 0.000640\n",
    "\n",
    "Test set: Average loss: 0.5182, Accuracy: 9924/10000 (99%)\n",
    "\n",
    "Train Epoch: 39 [0/60000 (0%)]\tLoss: 0.000504\n",
    "Train Epoch: 39 [40000/60000 (67%)]\tLoss: 0.000032\n",
    "\n",
    "Test set: Average loss: 0.4781, Accuracy: 9931/10000 (99%)\n",
    "\n",
    "Train Epoch: 40 [0/60000 (0%)]\tLoss: 0.024756\n",
    "Train Epoch: 40 [40000/60000 (67%)]\tLoss: 0.025174\n",
    "\n",
    "Test set: Average loss: 0.5059, Accuracy: 9927/10000 (99%)\n",
    "\n",
    "Train Epoch: 41 [0/60000 (0%)]\tLoss: 0.000009\n",
    "Train Epoch: 41 [40000/60000 (67%)]\tLoss: 0.000225\n",
    "\n",
    "Test set: Average loss: 0.4842, Accuracy: 9928/10000 (99%)\n",
    "\n",
    "Train Epoch: 42 [0/60000 (0%)]\tLoss: 0.179944\n",
    "Train Epoch: 42 [40000/60000 (67%)]\tLoss: 0.000029\n",
    "\n",
    "Test set: Average loss: 0.4923, Accuracy: 9927/10000 (99%)\n",
    "\n",
    "Train Epoch: 43 [0/60000 (0%)]\tLoss: 0.000369\n",
    "Train Epoch: 43 [40000/60000 (67%)]\tLoss: 0.001200\n",
    "\n",
    "Test set: Average loss: 0.5420, Accuracy: 9926/10000 (99%)\n",
    "\n",
    "Train Epoch: 44 [0/60000 (0%)]\tLoss: 0.000029\n",
    "Train Epoch: 44 [40000/60000 (67%)]\tLoss: 0.000030\n",
    "\n",
    "Test set: Average loss: 0.5266, Accuracy: 9921/10000 (99%)\n",
    "\n",
    "Train Epoch: 45 [0/60000 (0%)]\tLoss: 0.000003\n",
    "Train Epoch: 45 [40000/60000 (67%)]\tLoss: 0.014926\n",
    "\n",
    "Test set: Average loss: 0.5638, Accuracy: 9924/10000 (99%)\n",
    "\n",
    "Train Epoch: 46 [0/60000 (0%)]\tLoss: 0.002075\n",
    "Train Epoch: 46 [40000/60000 (67%)]\tLoss: 0.268330\n",
    "\n",
    "Test set: Average loss: 0.5043, Accuracy: 9932/10000 (99%)\n",
    "\n",
    "Train Epoch: 47 [0/60000 (0%)]\tLoss: 0.000662\n",
    "Train Epoch: 47 [40000/60000 (67%)]\tLoss: 0.000080\n",
    "\n",
    "Test set: Average loss: 0.4993, Accuracy: 9933/10000 (99%)\n",
    "\n",
    "Train Epoch: 48 [0/60000 (0%)]\tLoss: 0.003393\n",
    "Train Epoch: 48 [40000/60000 (67%)]\tLoss: 0.000003\n",
    "\n",
    "Test set: Average loss: 0.5199, Accuracy: 9930/10000 (99%)\n",
    "\n",
    "Train Epoch: 49 [0/60000 (0%)]\tLoss: 0.000946\n",
    "Train Epoch: 49 [40000/60000 (67%)]\tLoss: 0.000003\n",
    "\n",
    "Test set: Average loss: 0.5167, Accuracy: 9926/10000 (99%)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=[]\n",
    "acc=[]\n",
    "loss2=[]\n",
    "\n",
    "for i in txt.split('\\n'):\n",
    "     if len(i)>0:\n",
    "        if '40000/50000 ' in i:\n",
    "            loss.append(float(i.split()[-1].strip()))\n",
    "        if 'Test' in i:\n",
    "            loss2.append(float(i.split()[4].strip().replace(\",\", \"\")))\n",
    "            acc.append(float(i.split()[-2].split('/')[0])/10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\syftpy\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "D:\\Anaconda\\envs\\syftpy\\lib\\site-packages\\numpy\\.libs\\libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll\n",
      "D:\\Anaconda\\envs\\syftpy\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (50,) and (0,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mD:\\Temp\\ipykernel_7212\\1948476892.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[0mplotP\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Temp\\ipykernel_7212\\1948476892.py\u001b[0m in \u001b[0;36mplotP\u001b[1;34m(test_loss, train_loss, train_acc_list, test_acc_list)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_acc_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"train_loss\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"test_loss\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\syftpy\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2767\u001b[0m     return gca().plot(\n\u001b[0;32m   2768\u001b[0m         \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2769\u001b[1;33m         **({\"data\": data} if data is not None else {}), **kwargs)\n\u001b[0m\u001b[0;32m   2770\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2771\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\syftpy\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1633\u001b[0m         \"\"\"\n\u001b[0;32m   1634\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1635\u001b[1;33m         \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1636\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1637\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\syftpy\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m    310\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m                 \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m             \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\syftpy\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[1;34m(self, tup, kwargs, return_kwargs)\u001b[0m\n\u001b[0;32m    496\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    497\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 498\u001b[1;33m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[0;32m    499\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[0;32m    500\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (50,) and (0,)"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0UAAAGPCAYAAACTa3tOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAeHUlEQVR4nO3db2zdVf3A8U/b0VuItAzn2m0WJyigAhturBYkBFNpIhnugaEOsi0LiMgkQKOy8WcV0XUqkCVSXBggPsENCRDCliJUFqLULG5rAnEbwTG2ENptKu0surL2+3tgqL+6Dna7/qE7r1dyH/Rwzv2eSw6DN9/bewuyLMsCAAAgUYVjvQEAAICxJIoAAICkiSIAACBpoggAAEiaKAIAAJImigAAgKSJIgAAIGmiCAAASJooAgAAkiaKAACApOUdRS+99FLMnTs3pk6dGgUFBfH0009/6JqNGzfGF7/4xcjlcvGZz3wmHn300SFsFQAAYPjlHUXd3d0xY8aMaGpqOqr5b7zxRlx++eVx6aWXRltbW9x8881x7bXXxnPPPZf3ZgEAAIZbQZZl2ZAXFxTEU089FfPmzTvinFtvvTXWr18fr776av/YN7/5zXjnnXeiubl5qJcGAAAYFhNG+gKtra1RU1MzYKy2tjZuvvnmI645ePBgHDx4sP/nvr6++Pvf/x4f//jHo6CgYKS2CgAAfMRlWRYHDhyIqVOnRmHh8HxEwohHUXt7e5SXlw8YKy8vj66urvjXv/4VJ5544mFrGhsb46677hrprQEAAOPUnj174pOf/OSwPNeIR9FQLFu2LOrr6/t/7uzsjNNOOy327NkTpaWlY7gzAABgLHV1dUVlZWWcfPLJw/acIx5FFRUV0dHRMWCso6MjSktLB71LFBGRy+Uil8sdNl5aWiqKAACAYf21mhH/nqLq6upoaWkZMPb8889HdXX1SF8aAADgQ+UdRf/85z+jra0t2traIuI/H7nd1tYWu3fvjoj/vPVt4cKF/fOvv/762LlzZ/zgBz+I7du3xwMPPBCPP/543HLLLcPzCgAAAI5B3lH05z//Oc4///w4//zzIyKivr4+zj///Fi+fHlERLz99tv9gRQR8elPfzrWr18fzz//fMyYMSPuvffeeOihh6K2tnaYXgIAAMDQHdP3FI2Wrq6uKCsri87OTr9TBAAACRuJNhjx3ykCAAD4KBNFAABA0kQRAACQNFEEAAAkTRQBAABJE0UAAEDSRBEAAJA0UQQAACRNFAEAAEkTRQAAQNJEEQAAkDRRBAAAJE0UAQAASRNFAABA0kQRAACQNFEEAAAkTRQBAABJE0UAAEDSRBEAAJA0UQQAACRNFAEAAEkTRQAAQNJEEQAAkDRRBAAAJE0UAQAASRNFAABA0kQRAACQNFEEAAAkTRQBAABJE0UAAEDSRBEAAJA0UQQAACRNFAEAAEkTRQAAQNJEEQAAkDRRBAAAJE0UAQAASRNFAABA0kQRAACQNFEEAAAkTRQBAABJE0UAAEDSRBEAAJA0UQQAACRNFAEAAEkTRQAAQNJEEQAAkDRRBAAAJE0UAQAASRNFAABA0kQRAACQNFEEAAAkTRQBAABJE0UAAEDSRBEAAJA0UQQAACRNFAEAAEkTRQAAQNJEEQAAkLQhRVFTU1NMnz49SkpKoqqqKjZt2vSB81etWhVnnXVWnHjiiVFZWRm33HJL/Pvf/x7ShgEAAIZT3lG0bt26qK+vj4aGhtiyZUvMmDEjamtrY+/evYPOf+yxx2Lp0qXR0NAQ27Zti4cffjjWrVsXt9122zFvHgAA4FjlHUX33XdffOtb34rFixfH5z//+Vi9enWcdNJJ8cgjjww6/+WXX46LLroorrrqqpg+fXpcdtllMX/+/A+9uwQAADAa8oqinp6e2Lx5c9TU1Pz3CQoLo6amJlpbWwddc+GFF8bmzZv7I2jnzp2xYcOG+NrXvnbE6xw8eDC6uroGPAAAAEbChHwm79+/P3p7e6O8vHzAeHl5eWzfvn3QNVdddVXs378/vvzlL0eWZXHo0KG4/vrrP/Dtc42NjXHXXXflszUAAIAhGfFPn9u4cWOsWLEiHnjggdiyZUs8+eSTsX79+rj77ruPuGbZsmXR2dnZ/9izZ89IbxMAAEhUXneKJk2aFEVFRdHR0TFgvKOjIyoqKgZdc+edd8aCBQvi2muvjYiIc889N7q7u+O6666L22+/PQoLD++yXC4XuVwun60BAAAMSV53ioqLi2PWrFnR0tLSP9bX1xctLS1RXV096Jp33333sPApKiqKiIgsy/LdLwAAwLDK605RRER9fX0sWrQoZs+eHXPmzIlVq1ZFd3d3LF68OCIiFi5cGNOmTYvGxsaIiJg7d27cd999cf7550dVVVW8/vrrceedd8bcuXP74wgAAGCs5B1FdXV1sW/fvli+fHm0t7fHzJkzo7m5uf/DF3bv3j3gztAdd9wRBQUFcccdd8Rbb70Vn/jEJ2Lu3Lnxk5/8ZPheBQAAwBAVZOPgPWxdXV1RVlYWnZ2dUVpaOtbbAQAAxshItMGIf/ocAADAR5koAgAAkiaKAACApIkiAAAgaaIIAABImigCAACSJooAAICkiSIAACBpoggAAEiaKAIAAJImigAAgKSJIgAAIGmiCAAASJooAgAAkiaKAACApIkiAAAgaaIIAABImigCAACSJooAAICkiSIAACBpoggAAEiaKAIAAJImigAAgKSJIgAAIGmiCAAASJooAgAAkiaKAACApIkiAAAgaaIIAABImigCAACSJooAAICkiSIAACBpoggAAEiaKAIAAJImigAAgKSJIgAAIGmiCAAASJooAgAAkiaKAACApIkiAAAgaaIIAABImigCAACSJooAAICkiSIAACBpoggAAEiaKAIAAJImigAAgKSJIgAAIGmiCAAASJooAgAAkiaKAACApIkiAAAgaaIIAABImigCAACSJooAAICkiSIAACBpoggAAEiaKAIAAJImigAAgKQNKYqamppi+vTpUVJSElVVVbFp06YPnP/OO+/EkiVLYsqUKZHL5eLMM8+MDRs2DGnDAAAAw2lCvgvWrVsX9fX1sXr16qiqqopVq1ZFbW1t7NixIyZPnnzY/J6envjqV78akydPjieeeCKmTZsWb775ZpxyyinDsX8AAIBjUpBlWZbPgqqqqrjgggvi/vvvj4iIvr6+qKysjBtvvDGWLl162PzVq1fHz3/+89i+fXuccMIJQ9pkV1dXlJWVRWdnZ5SWlg7pOQAAgPFvJNogr7fP9fT0xObNm6Ompua/T1BYGDU1NdHa2jrommeeeSaqq6tjyZIlUV5eHuecc06sWLEient7j3idgwcPRldX14AHAADASMgrivbv3x+9vb1RXl4+YLy8vDza29sHXbNz58544oknore3NzZs2BB33nln3HvvvfHjH//4iNdpbGyMsrKy/kdlZWU+2wQAADhqI/7pc319fTF58uR48MEHY9asWVFXVxe33357rF69+ohrli1bFp2dnf2PPXv2jPQ2AQCAROX1QQuTJk2KoqKi6OjoGDDe0dERFRUVg66ZMmVKnHDCCVFUVNQ/9rnPfS7a29ujp6cniouLD1uTy+Uil8vlszUAAIAhyetOUXFxccyaNStaWlr6x/r6+qKlpSWqq6sHXXPRRRfF66+/Hn19ff1jr732WkyZMmXQIAIAABhNeb99rr6+PtasWRO//vWvY9u2bfGd73wnuru7Y/HixRERsXDhwli2bFn//O985zvx97//PW666aZ47bXXYv369bFixYpYsmTJ8L0KAACAIcr7e4rq6upi3759sXz58mhvb4+ZM2dGc3Nz/4cv7N69OwoL/9talZWV8dxzz8Utt9wS5513XkybNi1uuummuPXWW4fvVQAAAAxR3t9TNBZ8TxEAABDxEfieIgAAgOONKAIAAJImigAAgKSJIgAAIGmiCAAASJooAgAAkiaKAACApIkiAAAgaaIIAABImigCAACSJooAAICkiSIAACBpoggAAEiaKAIAAJImigAAgKSJIgAAIGmiCAAASJooAgAAkiaKAACApIkiAAAgaaIIAABImigCAACSJooAAICkiSIAACBpoggAAEiaKAIAAJImigAAgKSJIgAAIGmiCAAASJooAgAAkiaKAACApIkiAAAgaaIIAABImigCAACSJooAAICkiSIAACBpoggAAEiaKAIAAJImigAAgKSJIgAAIGmiCAAASJooAgAAkiaKAACApIkiAAAgaaIIAABImigCAACSJooAAICkiSIAACBpoggAAEiaKAIAAJImigAAgKSJIgAAIGmiCAAASJooAgAAkiaKAACApIkiAAAgaaIIAABImigCAACSJooAAICkDSmKmpqaYvr06VFSUhJVVVWxadOmo1q3du3aKCgoiHnz5g3lsgAAAMMu7yhat25d1NfXR0NDQ2zZsiVmzJgRtbW1sXfv3g9ct2vXrvje974XF1988ZA3CwAAMNzyjqL77rsvvvWtb8XixYvj85//fKxevTpOOumkeOSRR464pre3N66++uq466674vTTTz+mDQMAAAynvKKop6cnNm/eHDU1Nf99gsLCqKmpidbW1iOu+9GPfhSTJ0+Oa6655qiuc/Dgwejq6hrwAAAAGAl5RdH+/fujt7c3ysvLB4yXl5dHe3v7oGv+8Ic/xMMPPxxr1qw56us0NjZGWVlZ/6OysjKfbQIAABy1Ef30uQMHDsSCBQtizZo1MWnSpKNet2zZsujs7Ox/7NmzZwR3CQAApGxCPpMnTZoURUVF0dHRMWC8o6MjKioqDpv/17/+NXbt2hVz587tH+vr6/vPhSdMiB07dsQZZ5xx2LpcLhe5XC6frQEAAAxJXneKiouLY9asWdHS0tI/1tfXFy0tLVFdXX3Y/LPPPjteeeWVaGtr639cccUVcemll0ZbW5u3xQEAAGMurztFERH19fWxaNGimD17dsyZMydWrVoV3d3dsXjx4oiIWLhwYUybNi0aGxujpKQkzjnnnAHrTznllIiIw8YBAADGQt5RVFdXF/v27Yvly5dHe3t7zJw5M5qbm/s/fGH37t1RWDiiv6oEAAAwbAqyLMvGehMfpqurK8rKyqKzszNKS0vHejsAAMAYGYk2cEsHAABImigCAACSJooAAICkiSIAACBpoggAAEiaKAIAAJImigAAgKSJIgAAIGmiCAAASJooAgAAkiaKAACApIkiAAAgaaIIAABImigCAACSJooAAICkiSIAACBpoggAAEiaKAIAAJImigAAgKSJIgAAIGmiCAAASJooAgAAkiaKAACApIkiAAAgaaIIAABImigCAACSJooAAICkiSIAACBpoggAAEiaKAIAAJImigAAgKSJIgAAIGmiCAAASJooAgAAkiaKAACApIkiAAAgaaIIAABImigCAACSJooAAICkiSIAACBpoggAAEiaKAIAAJImigAAgKSJIgAAIGmiCAAASJooAgAAkiaKAACApIkiAAAgaaIIAABImigCAACSJooAAICkiSIAACBpoggAAEiaKAIAAJImigAAgKSJIgAAIGmiCAAASJooAgAAkiaKAACApA0pipqammL69OlRUlISVVVVsWnTpiPOXbNmTVx88cUxceLEmDhxYtTU1HzgfAAAgNGUdxStW7cu6uvro6GhIbZs2RIzZsyI2tra2Lt376DzN27cGPPnz48XX3wxWltbo7KyMi677LJ46623jnnzAAAAx6ogy7IsnwVVVVVxwQUXxP333x8REX19fVFZWRk33nhjLF269EPX9/b2xsSJE+P++++PhQsXHtU1u7q6oqysLDo7O6O0tDSf7QIAAMeRkWiDvO4U9fT0xObNm6Ompua/T1BYGDU1NdHa2npUz/Huu+/Ge++9F6eeeuoR5xw8eDC6uroGPAAAAEZCXlG0f//+6O3tjfLy8gHj5eXl0d7eflTPceutt8bUqVMHhNX/amxsjLKysv5HZWVlPtsEAAA4aqP66XMrV66MtWvXxlNPPRUlJSVHnLds2bLo7Ozsf+zZs2cUdwkAAKRkQj6TJ02aFEVFRdHR0TFgvKOjIyoqKj5w7T333BMrV66MF154Ic4777wPnJvL5SKXy+WzNQAAgCHJ605RcXFxzJo1K1paWvrH+vr6oqWlJaqrq4+47mc/+1ncfffd0dzcHLNnzx76bgEAAIZZXneKIiLq6+tj0aJFMXv27JgzZ06sWrUquru7Y/HixRERsXDhwpg2bVo0NjZGRMRPf/rTWL58eTz22GMxffr0/t89+tjHPhYf+9jHhvGlAAAA5C/vKKqrq4t9+/bF8uXLo729PWbOnBnNzc39H76we/fuKCz87w2oX/7yl9HT0xPf+MY3BjxPQ0ND/PCHPzy23QMAAByjvL+naCz4niIAACDiI/A9RQAAAMcbUQQAACRNFAEAAEkTRQAAQNJEEQAAkDRRBAAAJE0UAQAASRNFAABA0kQRAACQNFEEAAAkTRQBAABJE0UAAEDSRBEAAJA0UQQAACRNFAEAAEkTRQAAQNJEEQAAkDRRBAAAJE0UAQAASRNFAABA0kQRAACQNFEEAAAkTRQBAABJE0UAAEDSRBEAAJA0UQQAACRNFAEAAEkTRQAAQNJEEQAAkDRRBAAAJE0UAQAASRNFAABA0kQRAACQNFEEAAAkTRQBAABJE0UAAEDSRBEAAJA0UQQAACRNFAEAAEkTRQAAQNJEEQAAkDRRBAAAJE0UAQAASRNFAABA0kQRAACQNFEEAAAkTRQBAABJE0UAAEDSRBEAAJA0UQQAACRNFAEAAEkTRQAAQNJEEQAAkDRRBAAAJE0UAQAASRNFAABA0kQRAACQNFEEAAAkTRQBAABJG1IUNTU1xfTp06OkpCSqqqpi06ZNHzj/t7/9bZx99tlRUlIS5557bmzYsGFImwUAABhueUfRunXror6+PhoaGmLLli0xY8aMqK2tjb179w46/+WXX4758+fHNddcE1u3bo158+bFvHnz4tVXXz3mzQMAAByrgizLsnwWVFVVxQUXXBD3339/RET09fVFZWVl3HjjjbF06dLD5tfV1UV3d3c8++yz/WNf+tKXYubMmbF69eqjumZXV1eUlZVFZ2dnlJaW5rNdAADgODISbTAhn8k9PT2xefPmWLZsWf9YYWFh1NTURGtr66BrWltbo76+fsBYbW1tPP3000e8zsGDB+PgwYP9P3d2dkbEf/4GAAAA6Xq/CfK8t/OB8oqi/fv3R29vb5SXlw8YLy8vj+3btw+6pr29fdD57e3tR7xOY2Nj3HXXXYeNV1ZW5rNdAADgOPW3v/0tysrKhuW58oqi0bJs2bIBd5feeeed+NSnPhW7d+8ethcOg+nq6orKysrYs2ePt2oyopw1Rouzxmhx1hgtnZ2dcdppp8Wpp546bM+ZVxRNmjQpioqKoqOjY8B4R0dHVFRUDLqmoqIir/kREblcLnK53GHjZWVl/iFjVJSWljprjApnjdHirDFanDVGS2Hh8H27UF7PVFxcHLNmzYqWlpb+sb6+vmhpaYnq6upB11RXVw+YHxHx/PPPH3E+AADAaMr77XP19fWxaNGimD17dsyZMydWrVoV3d3dsXjx4oiIWLhwYUybNi0aGxsjIuKmm26KSy65JO699964/PLLY+3atfHnP/85HnzwweF9JQAAAEOQdxTV1dXFvn37Yvny5dHe3h4zZ86M5ubm/g9T2L1794BbWRdeeGE89thjcccdd8Rtt90Wn/3sZ+Ppp5+Oc84556ivmcvloqGhYdC31MFwctYYLc4ao8VZY7Q4a4yWkThreX9PEQAAwPFk+H47CQAAYBwSRQAAQNJEEQAAkDRRBAAAJO0jE0VNTU0xffr0KCkpiaqqqti0adMHzv/tb38bZ599dpSUlMS5554bGzZsGKWdMt7lc9bWrFkTF198cUycODEmTpwYNTU1H3o24X35/rn2vrVr10ZBQUHMmzdvZDfIcSPfs/bOO+/EkiVLYsqUKZHL5eLMM8/071GOSr5nbdWqVXHWWWfFiSeeGJWVlXHLLbfEv//971HaLePRSy+9FHPnzo2pU6dGQUFBPP300x+6ZuPGjfHFL34xcrlcfOYzn4lHH3007+t+JKJo3bp1UV9fHw0NDbFly5aYMWNG1NbWxt69ewed//LLL8f8+fPjmmuuia1bt8a8efNi3rx58eqrr47yzhlv8j1rGzdujPnz58eLL74Yra2tUVlZGZdddlm89dZbo7xzxpt8z9r7du3aFd/73vfi4osvHqWdMt7le9Z6enriq1/9auzatSueeOKJ2LFjR6xZsyamTZs2yjtnvMn3rD322GOxdOnSaGhoiG3btsXDDz8c69ati9tuu22Ud8540t3dHTNmzIimpqajmv/GG2/E5ZdfHpdeemm0tbXFzTffHNdee20899xz+V04+wiYM2dOtmTJkv6fe3t7s6lTp2aNjY2Dzr/yyiuzyy+/fMBYVVVV9u1vf3tE98n4l+9Z+1+HDh3KTj755OzXv/71SG2R48RQztqhQ4eyCy+8MHvooYeyRYsWZV//+tdHYaeMd/metV/+8pfZ6aefnvX09IzWFjlO5HvWlixZkn3lK18ZMFZfX59ddNFFI7pPjh8RkT311FMfOOcHP/hB9oUvfGHAWF1dXVZbW5vXtcb8TlFPT09s3rw5ampq+scKCwujpqYmWltbB13T2to6YH5ERG1t7RHnQ8TQztr/evfdd+O9996LU089daS2yXFgqGftRz/6UUyePDmuueaa0dgmx4GhnLVnnnkmqqurY8mSJVFeXh7nnHNOrFixInp7e0dr24xDQzlrF154YWzevLn/LXY7d+6MDRs2xNe+9rVR2TNpGK4umDCcmxqK/fv3R29vb5SXlw8YLy8vj+3btw+6pr29fdD57e3tI7ZPxr+hnLX/deutt8bUqVMP+4cP/r+hnLU//OEP8fDDD0dbW9so7JDjxVDO2s6dO+P3v/99XH311bFhw4Z4/fXX44Ybboj33nsvGhoaRmPbjENDOWtXXXVV7N+/P7785S9HlmVx6NChuP766719jmF1pC7o6uqKf/3rX3HiiSce1fOM+Z0iGC9WrlwZa9eujaeeeipKSkrGejscRw4cOBALFiyINWvWxKRJk8Z6Oxzn+vr6YvLkyfHggw/GrFmzoq6uLm6//fZYvXr1WG+N48zGjRtjxYoV8cADD8SWLVviySefjPXr18fdd9891luDw4z5naJJkyZFUVFRdHR0DBjv6OiIioqKQddUVFTkNR8ihnbW3nfPPffEypUr44UXXojzzjtvJLfJcSDfs/bXv/41du3aFXPnzu0f6+vri4iICRMmxI4dO+KMM84Y2U0zLg3lz7UpU6bECSecEEVFRf1jn/vc56K9vT16enqiuLh4RPfM+DSUs3bnnXfGggUL4tprr42IiHPPPTe6u7vjuuuui9tvvz0KC/2/eY7dkbqgtLT0qO8SRXwE7hQVFxfHrFmzoqWlpX+sr68vWlpaorq6etA11dXVA+ZHRDz//PNHnA8RQztrERE/+9nP4u67747m5uaYPXv2aGyVcS7fs3b22WfHK6+8Em1tbf2PK664ov+TdCorK0dz+4wjQ/lz7aKLLorXX3+9P7wjIl577bWYMmWKIOKIhnLW3n333cPC5/0Y/8/v0MOxG7YuyO8zIEbG2rVrs1wulz366KPZX/7yl+y6667LTjnllKy9vT3LsixbsGBBtnTp0v75f/zjH7MJEyZk99xzT7Zt27asoaEhO+GEE7JXXnllrF4C40S+Z23lypVZcXFx9sQTT2Rvv/12/+PAgQNj9RIYJ/I9a//Lp89xtPI9a7t3785OPvnk7Lvf/W62Y8eO7Nlnn80mT56c/fjHPx6rl8A4ke9Za2hoyE4++eTsN7/5TbZz587sd7/7XXbGGWdkV1555Vi9BMaBAwcOZFu3bs22bt2aRUR23333ZVu3bs3efPPNLMuybOnSpdmCBQv65+/cuTM76aSTsu9///vZtm3bsqampqyoqChrbm7O67ofiSjKsiz7xS9+kZ122mlZcXFxNmfOnOxPf/pT/1+75JJLskWLFg2Y//jjj2dnnnlmVlxcnH3hC1/I1q9fP8o7ZrzK56x96lOfyiLisEdDQ8Pob5xxJ98/1/4/UUQ+8j1rL7/8clZVVZXlcrns9NNPz37yk59khw4dGuVdMx7lc9bee++97Ic//GF2xhlnZCUlJVllZWV2ww03ZP/4xz9Gf+OMGy+++OKg/+31/tlatGhRdskllxy2ZubMmVlxcXF2+umnZ7/61a/yvm5Blrl/CQAApGvMf6cIAABgLIkiAAAgaaIIAABImigCAACSJooAAICkiSIAACBpoggAAEiaKAIAAJImigAAgKSJIgAAIGmiCAAASJooAgAAkvZ/d4D9zBEZgCkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# mpl.use('nbAgg')\n",
    "# mpl.style.use('seaborn-darkgrid')\n",
    "import numpy as np\n",
    "def plotP(test_loss, train_loss, train_acc_list, test_acc_list):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    x = np.linspace(0,  len(train_loss))\n",
    "    y = np.linspace(0, len(test_acc_list))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(x, train_loss, label=\"train_loss\")\n",
    "    plt.plot(x, test_loss, label=\"test_loss\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.legend()\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(y, test_acc_list, label=\"test_acc\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"acc\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plotP(loss2, loss, [], acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "PyCharm (000--code)",
   "language": "python",
   "name": "pycharm-c23f549c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "785px",
    "left": "47px",
    "top": "355px",
    "width": "179.4px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
